<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Machine Learning Quiz 2 Practice | Sachin Yadav</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Machine Learning Quiz 2 Practice" />
<meta name="author" content="Sachin Yadav" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Sachin Yadav’s blog." />
<meta property="og:description" content="Sachin Yadav’s blog." />
<link rel="canonical" href="https://yadav-sachin.github.io/blog/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html" />
<meta property="og:url" content="https://yadav-sachin.github.io/blog/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html" />
<meta property="og:site_name" content="Sachin Yadav" />
<meta property="og:image" content="https://c.tenor.com/cu2Gonk18tEAAAAC/nerd-sponge-bob.gif" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-02-17T00:00:00-06:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://c.tenor.com/cu2Gonk18tEAAAAC/nerd-sponge-bob.gif" />
<meta property="twitter:title" content="Machine Learning Quiz 2 Practice" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Sachin Yadav"},"dateModified":"2022-02-17T00:00:00-06:00","datePublished":"2022-02-17T00:00:00-06:00","description":"Sachin Yadav’s blog.","headline":"Machine Learning Quiz 2 Practice","image":"https://c.tenor.com/cu2Gonk18tEAAAAC/nerd-sponge-bob.gif","mainEntityOfPage":{"@type":"WebPage","@id":"https://yadav-sachin.github.io/blog/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html"},"url":"https://yadav-sachin.github.io/blog/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://yadav-sachin.github.io/blog/feed.xml" title="Sachin Yadav" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" /><script src="https://hypothes.is/embed.js" async></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Sachin Yadav</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Machine Learning Quiz 2 Practice</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-02-17T00:00:00-06:00" itemprop="datePublished">
        Feb 17, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Sachin Yadav</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      11 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#MLCourse2022">MLCourse2022</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/yadav-sachin/blog/tree/master/_notebooks/2022-02-17-machine-learning-quiz2-practice.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/yadav-sachin/blog/master?filepath=_notebooks%2F2022-02-17-machine-learning-quiz2-practice.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/yadav-sachin/blog/blob/master/_notebooks/2022-02-17-machine-learning-quiz2-practice.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fyadav-sachin%2Fblog%2Fblob%2Fmaster%2F_notebooks%2F2022-02-17-machine-learning-quiz2-practice.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/blog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#Maths-for-ML">Maths for ML </a></li>
<li class="toc-entry toc-h2"><a href="#Introduction-to-Linear-Regression">Introduction to Linear Regression </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Normal-Equation">Normal Equation </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Geometric-Interpretation-of-Linear-Regression">Geometric Interpretation of Linear Regression </a></li>
<li class="toc-entry toc-h2"><a href="#Linear-Regression">Linear Regression </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Relation-between-#samples-and-#features-in-Linear-Regression">Relation between #samples and #features in Linear Regression </a></li>
<li class="toc-entry toc-h3"><a href="#Variable-Transformation">Variable Transformation </a></li>
<li class="toc-entry toc-h3"><a href="#Multi-collinearity">Multi-collinearity </a></li>
<li class="toc-entry toc-h3"><a href="#Dummy-Variables">Dummy Variables </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Convexity">Convexity </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Prove-$f(x)-=-x^{2}$-is-Convex">Prove $f(x) = x^{2}$ is Convex </a></li>
<li class="toc-entry toc-h3"><a href="#Double-Derivative-Test">Double-Derivative Test </a></li>
<li class="toc-entry toc-h3"><a href="#Convexity-of-Least-Squares-in-Linear-Regression">Convexity of Least Squares in Linear Regression </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Gradient-Descent">Gradient Descent </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Contour-Plots-and-Gradients">Contour Plots and Gradients </a></li>
<li class="toc-entry toc-h3"><a href="#Algorithm">Algorithm </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-02-17-machine-learning-quiz2-practice.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">ipywidgets</span> <span class="kn">import</span> <span class="n">interact</span><span class="p">,</span> <span class="n">interactive</span><span class="p">,</span> <span class="n">fixed</span><span class="p">,</span> <span class="n">interact_manual</span>
<span class="kn">import</span> <span class="nn">ipywidgets</span> <span class="k">as</span> <span class="nn">widgets</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Maths-for-ML">
<a class="anchor" href="#Maths-for-ML" aria-hidden="true"><span class="octicon octicon-link"></span></a>Maths for ML<a class="anchor-link" href="#Maths-for-ML"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>Given a vector $\epsilon$, we can calculate $\sum\epsilon_{i}^{2}$ using $\epsilon^{T} \epsilon$</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># As per the convention we take epsilon to be a 2D column vector</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">6.4</span><span class="p">,</span> <span class="mf">8.1</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">5.9</span><span class="p">,</span> <span class="mf">8.5</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span>

<span class="n">epsilon_square_sum1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">epsilon</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">epsilon_square_sum2</span> <span class="o">=</span> <span class="p">(</span><span class="n">epsilon</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">epsilon</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">epsilon_square_sum1</span><span class="p">,</span> <span class="n">epsilon_square_sum2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>$(AB)^{T} = B^{T}A^{T}$</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="n">ab_t</span> <span class="o">=</span> <span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">B</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">b_t_a_t</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ab_t</span><span class="p">,</span> <span class="n">b_t_a_t</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>For a scalar $s$, $s = s^{T}$</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>Derivative of scalar $s$ with respect to (yes!, I wrote wrt as full here 😁) vector $\theta$
$$\theta = \begin{bmatrix} \theta_{1} \\ \theta_{2} \\ \vdots \\ \theta{n} \end{bmatrix}$$
 $$\frac{\partial s}{\partial \theta} = \begin{bmatrix}
  \frac{\partial s}{\partial \theta_{1}} \\
  \frac{\partial s}{\partial \theta_{2}} \\
  \frac{\partial s}{\partial \theta_{3}} \\
  \vdots \\
  \frac{\partial s}{\partial \theta_{n}} 
  \end{bmatrix} $$</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>If $A$ is a matrix and $\theta$ is a vector, and $A\theta$ is a scalar. Then 
$$ \frac{\partial A \theta}{\partial \theta} = A^{T} $$</li>
</ol>
<p>🤔 Taking some similarity with $a\theta$, where both $a$ and $\theta$ are scalar, I have an idea that it would be A. But shape of gradient would be $N \times 1$, so $A^{T}$ is my guess before starting any calculations.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="c1"># as A $\theta$ is scalar, so A.shape[0] should be 1.</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">scalar</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">theta</span>
<span class="n">scalar</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ol>
<li>Assume $Z$ is a matrix of form $X^{T}X$, then 
$$ \frac{\partial (\theta^{T}Z\theta)}{\partial \theta} = 2Z^{T}\theta$$</li>
</ol>
<p>🤔 Let me again make a good guess before any calculation, if $\theta$ and $Z$ are both scaler, then the derivative would look like $2Z\theta$. So my guess would $2Z\theta$, which is equal to $2Z^{T}\theta$ as both are $Z$ is symmetric.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">scalar</span> <span class="o">=</span> <span class="n">theta</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Z</span> <span class="o">@</span> <span class="n">theta</span>
<span class="n">scalar</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="k">assert</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">Z</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">theta</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's skip over the content of Rank topic for now.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The maximum rank possible for a matrix is $max(R, C)$</p>
<p>But an interesting question would be 🤔, what is the minimum rank possible for a matrix, is it 0, is it 1?</p>
<p>Ans: Rank is zero, in case of zero matrix.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Just a leaving thought, if I would have been a developer of Numpy, I would not have allowed <code>np.eye</code> as the method for identity matrix. Better to use <code>np.identity</code> only. 😞</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Introduction-to-Linear-Regression">
<a class="anchor" href="#Introduction-to-Linear-Regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction to Linear Regression<a class="anchor-link" href="#Introduction-to-Linear-Regression"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Considering <code>weight</code> as a linear function of <code>height</code>:</p>
<ul>
<li>$weight_{1} \approx \theta_{0} + \theta_{1} * height_{1}$</li>
<li>$weight_{2} \approx \theta_{0} + \theta_{1} * height_{2}$</li>
<li>$weight_{N} \approx \theta_{0} + \theta_{1} * height_{N}$</li>
</ul>
<p>Add extra columns of $1s$ for the bias term in $\theta$</p>
<p>
$$ W_{N\times1} = X_{N\times2} \, \theta_{2\times1} $$

where the feature matrix $X$, $X = \begin{bmatrix}
1 &amp; height_{1} \\
1 &amp; height_{2} \\
\vdots &amp; \vdots \\
1 &amp; height_{N}
\end{bmatrix}$</p>
<ul>
<li>$\theta_{0}$, Bias/Intercept term : (the value of $y$, when $x$ is set to zero)</li>
<li>$\theta_{1}$, Slope term : (the increase in $y$, when $x$ is increased by 1 unit)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Generalized Linear Regression</strong></p>
<ul>
<li>$N$: Number of training samples</li>
<li>$M$: Number of features</li>
</ul>
$$ \begin{bmatrix}
\hat{y}_{1} \\
\hat{y}_{2} \\
\vdots \\
\hat{y}_{N} \\
\end{bmatrix}
_{N \times 1}
= \begin{bmatrix}
1 &amp; x_{1, 1} &amp; x_{1, 2} &amp; \ldots &amp; x_{1, M} \\
1 &amp; x_{2, 1} &amp; x_{2, 2} &amp; \ldots &amp; x_{2, M} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ldots &amp; \vdots \\
1 &amp; x_{N, 1} &amp; x_{N, 2} &amp; \ldots &amp; x_{N, M} \\
\end{bmatrix} _{N \times (M + 1)}
\begin{bmatrix}
\theta_{0} \\
\theta_{1} \\
\vdots \\
\theta_{M}
\end{bmatrix} _{(M + 1)\times 1}
$$<p>
$$ \hat{y} = X \theta $$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, the task at our hand is to estimate "good" values of $\theta$, which will give "good" approximation to the actual values.But how do we decide if a set of values of $\theta$ is "better" than another value of $\theta$. We need a metric for evalution here.</p>
<p>Let $\epsilon_{i}$ be $y_{i} - \hat{y}_{i}$, where $\epsilon_{i} \sim \mathcal{N} (0, \sigma^{2})$. We are assuming that $\epsilon_{i}$ is coming from this normal distribution.</p>
<p>We want $|\epsilon_{1}|$, $|\epsilon_{2}|$, $|\epsilon_{3}|$ ... , $|\epsilon_{N}|$ to be small.</p>
<p>So we can try to minimize L2 norm (Squared Error) or L1 norm.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">weight_height_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">"https://raw.githubusercontent.com/yadav-sachin/blog/master/_notebooks/assets/2022-02-17-machine-learning-quiz2-practice/weight-height.csv"</span>
<span class="p">)</span>
<span class="c1"># take 30 points</span>
<span class="n">sampled_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weight_height_df</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">weight_height_df</span> <span class="o">=</span> <span class="n">weight_height_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">sampled_idx</span><span class="p">][[</span><span class="s2">"Height"</span><span class="p">,</span> <span class="s2">"Weight"</span><span class="p">]]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span>
    <span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s2">"Height"</span><span class="p">]</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_func</span><span class="p">(</span><span class="n">theta0</span><span class="p">,</span> <span class="n">theta1</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">weight_height_df</span><span class="p">[</span><span class="s2">"Height"</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">weight_height_df</span><span class="p">[</span><span class="s2">"Weight"</span><span class="p">]</span>
    <span class="n">y_hat</span> <span class="o">=</span> <span class="n">theta0</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">theta1</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Actual"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">"Pred"</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">"--"</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">400</span><span class="p">])</span>
    <span class="n">mse_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s2">"$\theta_</span><span class="si">{</span><span class="mi">0</span><span class="si">}</span><span class="s2">$=</span><span class="si">{</span><span class="n">theta0</span><span class="si">}</span><span class="s2">, $\theta_</span><span class="si">{</span><span class="mi">1</span><span class="si">}</span><span class="s2">$=</span><span class="si">{</span><span class="n">theta1</span><span class="si">}</span><span class="s2">    MSE val: </span><span class="si">{</span><span class="n">mse_val</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>


<span class="n">interact</span><span class="p">(</span>
    <span class="n">plot_func</span><span class="p">,</span>
    <span class="n">theta0</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s2">"theta0 (bias)"</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mi">300</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">1000</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">theta1</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s2">"theta1 (slope)"</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">7.5</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">20</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.01</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;function __main__.plot_func(theta0, theta1)&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>Run the notebook in Colab to view the interactive plot above, where we manually change parameters (using sliders) and fit the line through training points with Mean Squared error as the guiding value.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Normal-Equation">
<a class="anchor" href="#Normal-Equation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normal Equation<a class="anchor-link" href="#Normal-Equation"> </a>
</h3>
<p>
$$ y = X\theta + \epsilon$$

Objective: To minimize $\epsilon^{T} \epsilon$

$$\epsilon^{T} \epsilon = y y^{T} - 2 y^{T}X\theta + \theta^{T}X^{T}X\theta$$

$$\frac{\partial (\epsilon^{T} \epsilon)}{\partial \theta} = -2X^{T}y + 2X^{T}X\theta$$ 
(we use some of our results from previous chapter "Maths for ML")</p>
<p>Setting it to zero, 

$$ \theta^{*} = (X^{T}X)^{-1}X^{T}y$$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Geometric-Interpretation-of-Linear-Regression">
<a class="anchor" href="#Geometric-Interpretation-of-Linear-Regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Geometric Interpretation of Linear Regression<a class="anchor-link" href="#Geometric-Interpretation-of-Linear-Regression"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have $\hat{y} = X\theta$, where $X$ is shape of $(N \times M)$, where $M$ is #features and $N$ is #samples.</p>
<p>When we multiply $X$ with column vector $\theta$, the get a column vector which is the linear combination of the columns of matrix $X$. The linear combination, the coeffients of combination are decided by the parameters in $\theta$. So $X\theta$ lies in the span of columns of X.</p>
<p>Our objective is to get a $\theta$ to minimize $\mathopen|| y - X\theta \mathclose||$.</p>
<p>The span of columns of $X$ would be a plane in $N$ dimensional space, $X\theta$ lies on this plane. So the least distance, is when the $X\theta - y$ is perpendicular to this plane (span of columns of $X$).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Therefore $(y - \hat{y}) \bot (x_{j}) \forall j$

$$X^{T} (y - X\theta^{*}) = 0$$

We get, 

$$\theta^{*} = (X^{T}X)^{-1}X^{T}y$$
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Linear-Regression">
<a class="anchor" href="#Linear-Regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Regression<a class="anchor-link" href="#Linear-Regression"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Relation-between-#samples-and-#features-in-Linear-Regression">
<a class="anchor" href="#Relation-between-#samples-and-#features-in-Linear-Regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Relation between #samples and #features in Linear Regression<a class="anchor-link" href="#Relation-between-#samples-and-#features-in-Linear-Regression"> </a>
</h3>
<p>Let $N$ be number of samples and $M$ be the number of variables/features.</p>
<p><strong>Under-determined system</strong>: In Linear Algebra, if we have $N$ equations with $M$ variables and $N &lt; M$, then it is called an under-determined system. In this case, we will have infinite solutions.</p>
<p><strong>Over-determined sytem</strong>: When $N &gt; M$, the system is over-determined. In this case the sum of residuals $\sum \mathopen| \epsilon_{i} \mathclose| &gt; 0$, in most cases unless we are able to get perfect fit.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Variable-Transformation">
<a class="anchor" href="#Variable-Transformation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Variable Transformation<a class="anchor-link" href="#Variable-Transformation"> </a>
</h3>
<p>In case the output does not seem like a linear combination of the variables, we can also use the higher powers of variables in the linear combination.</p>
<p>We can also use other transformations like logarithm, multiples of more than one variable, etc. And we would still call it "Linear" Regression, as we are here talking about the linearity in coefficients/parameter space ($\theta$).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multi-collinearity">
<a class="anchor" href="#Multi-collinearity" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multi-collinearity<a class="anchor-link" href="#Multi-collinearity"> </a>
</h3>
<p>There can be situations when $X^{T}X$ is a singular matrix. such as 
$$ X = \begin{bmatrix} 
1 &amp; 1 &amp; 2 \\
1 &amp; 2 &amp; 4 \\
1 &amp; 3 &amp; 6 
\end{bmatrix}$$
For 
$$\begin{bmatrix} x_{1} &amp; x_{2} &amp; y \\
1 &amp; 2 &amp; 4 \\
2 &amp; 4 &amp; 6 \\
3 &amp; 6 &amp; 8 \end{bmatrix} $$</p>
<p>In this case, a perfect fit is not possible.</p>
<p>In case, we still want to use the normal equation. 
The ways are:</p>
<ul>
<li>Regularise: We can add some jitter/noise to the diagonal values and make it invertible.</li>
<li>Drop Variables: As here in this case, $x_{2} = 2 * x_{1}$, so we may drop either one of these variables.</li>
<li>Using different subsets of the data samples might work</li>
<li>Avoid dummy variable trap</li>
</ul>
<p>This problem also arises due to dummy variable trap.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Dummy-Variables">
<a class="anchor" href="#Dummy-Variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dummy Variables<a class="anchor-link" href="#Dummy-Variables"> </a>
</h3>
<p>Let's assume if we have a categorical variable in linear regression setup, where the air pollution is a function of (#Vehicles, Wind-speed and Wind-direction). As we cannot give categorical values, we need to have corresponding numerical values.</p>
<p>Such as if we have <code>wind-direction</code>, there are 4 categories of this variables namely <code>North</code>, <code>West</code>, <code>East</code> and <code>South</code>.
If we go ahead and numerically encode them, <code>North</code> values become $0$, <code>West</code> values become $1$ and so on.</p>
<p>The problem here is that we cannot assign such numerical ordering to the categories, as numerical categories have comparion in case of values. Such as $2$ = $2 \times 1$, but is contribution due to  <code>West</code> = $2 \times$ <code>North</code></p>
<p><strong>N-1 Variable Encoding</strong>: Then to avoid this problem, we use <strong>$C-1$ variable encodings</strong>, where $C$ is the number of the categories of the categorical variable.
The logic is that, to specify the class of the sample, I need to ask at least $C-1$ binary questions.</p>
<ul>
<li>Is it Class $1$?</li>
<li>Is it Class $2$?</li>
<li>$\vdots$</li>
<li>Is it Class $C-1$?</li>
</ul>
<p><strong>One-hot Variable Encoding</strong>: I can ask $C$ binary questions.</p>
<ul>
<li>Is it Class $1$?</li>
<li>Is it Class $2$?</li>
<li>$\vdots$</li>
<li>Is it Class $C - 1$?</li>
<li>Is it Class $C$?</li>
</ul>
<p><strong>But which one is better?</strong>
The $C_{th}$ class variable is redundant, as we can determine its values based on $C-1$ class variables.</p>
<p>So One-hot encoding can cause multi-collinearity in Linear Regression, as one of the added variables is redundant and dependent on other variables.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Convexity">
<a class="anchor" href="#Convexity" aria-hidden="true"><span class="octicon octicon-link"></span></a>Convexity<a class="anchor-link" href="#Convexity"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Convexity defined over an interval $[\alpha, \beta]$, is such that the line segment joining two points $(a, f(a))$, $(b, f(b))$ is above or on the function for all points between in $[a, b]$, given $\alpha \leq a, b \leq \beta$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One of the problem I noticed is that students/people are just giving out more complex definitions of "convexity", which makes the things look complex but are actually very simple. So let's not discuss: other definitions reducing distance, Rolle's theroem, etc. and keep the definition simple.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Prove-$f(x)-=-x^{2}$-is-Convex">
<a class="anchor" href="#Prove-%24f(x)-=-x%5E%7B2%7D%24-is-Convex" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prove $f(x) = x^{2}$ is Convex<a class="anchor-link" href="#Prove-%24f(x)-=-x%5E%7B2%7D%24-is-Convex"> </a>
</h3>
<p>Let the two chosen points are $(a, f(a))$ and $(b, f(b))$.</p>
<p>Then any point on the line segment joining the points can be described as $$(t \times a + (1 - t) \times b, t\times f(a) + (1 - t) \times f(b))$$</p>
<p>the corresponding point on the curve at the same $x$-coordinate is 
$$(t \times a + (1 - t) \times b, f(t \times a + (1 - t) \times b))$$ 
here $ 0 &lt; t &lt; 1$</p>
<p>The difference in $y$ is:

$$t(1-t)(a - b)^{2}$$

which is greater than or equal to zero. Hence the line-segment is always above the function in all points between $a$ and $b$. So function is convex.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Double-Derivative-Test">
<a class="anchor" href="#Double-Derivative-Test" aria-hidden="true"><span class="octicon octicon-link"></span></a>Double-Derivative Test<a class="anchor-link" href="#Double-Derivative-Test"> </a>
</h3>
<p>If double-derivative wrt x &gt; 0, then convex.</p>
<p><strong>Double-Derivative Test for multi-parameter function</strong>
It is equal to Hessian matrix. 
A function $f(x_{1}, x_{2}, x_{3}, \ldots, x_{n})$ is convex iff Hessian Matrix is positive semidefinite for all possible values of $(x_{1}, x_{2}, x_{3}, \ldots, x_{n})$.</p>
$$(Hess f)_{ij} \equiv \frac{\partial^{2} f}{\partial x_{i} \partial x_{j} } $$<p>
The Hessian matrix is of shape $(N \times N)$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let $f(x_{1}, x_{2}) = x_{1}^{2} + x_{2}^{2}$</p>
<p>
$$ Hess(f) = \begin{bmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{bmatrix} $$

This is positive semidefinite, so $x_{1}^{2} + x_{2}^{2}$ is convex function.
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z"></path></svg>
    <strong>Tip: </strong>A quick way to check if a matrix is semi-positive definite is that you take determinant of 1X1, 2X2, ..., NXN upper sub-matrices containing the first pivot point and check if greater than or equal to zero.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Convexity-of-Least-Squares-in-Linear-Regression">
<a class="anchor" href="#Convexity-of-Least-Squares-in-Linear-Regression" aria-hidden="true"><span class="octicon octicon-link"></span></a>Convexity of Least Squares in Linear Regression<a class="anchor-link" href="#Convexity-of-Least-Squares-in-Linear-Regression"> </a>
</h3>
<p>
$$f(\theta) = \mathopen|| y - X\theta \mathclose||^{2}$$

This is equal to $y y^{T} - 2 y^{T}X\theta + \theta^{T}X^{T}X\theta$.</p>
<p>We double differentiate it and get $X^{T}X$, which is a positive semidefinite matrix.</p>
<p>So this is where we connect convexity to Linear Regression.</p>
<p><img src="https://media1.giphy.com/media/rOEvmLAxxcE1i/giphy.gif" alt='"Spongebob: Excited"'></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If $f(x)$ and $g(x)$ is convex,</p>
<ul>
<li>$f(x) + g(x)$ is convex.</li>
<li>$kf(x)$ is convex, for $k \geq 0$.</li>
</ul>
<p>Convex function has this unique property of having a unique minima, which is the global minima.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Gradient-Descent">
<a class="anchor" href="#Gradient-Descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Descent<a class="anchor-link" href="#Gradient-Descent"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Contour-Plots-and-Gradients">
<a class="anchor" href="#Contour-Plots-and-Gradients" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contour Plots and Gradients<a class="anchor-link" href="#Contour-Plots-and-Gradients"> </a>
</h3>
<p>Gradients is the direction for maximum increase in the function value.</p>
<p>To decrease the value of the function the most, we move in the opposite direction of gradient (negative of gradient vector).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Algorithm">
<a class="anchor" href="#Algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algorithm<a class="anchor-link" href="#Algorithm"> </a>
</h3>
<ul>
<li>Start with some $x_{0}$</li>
<li>Till convergence or iterations exhausted<ul>
<li>$x_{i} = x_{i - 1} - \alpha\frac{\partial f(x)}{\partial x}$, $\alpha$ is the step-size or learning-rate</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="yadav-sachin/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Sachin Yadav&#39;s blog.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" target="_blank" title="fastai"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" target="_blank" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
