{
  
    
        "post0": {
            "title": "Title",
            "content": "- toc: true - badges: true - comments: true - author: Sachin Yadav - categories: [GRE] - image: https://i.redd.it/o89xi9drset51.png . Some Backstory . This week I started with my GRE preparation with GregMat.com. I will be posting a subset of my journey as my blogs. As sharing of GregMat premium content on other platforms is probhited and unethical, I will only refer to freely available content if required for revision. . GRE Exam Structure .",
            "url": "https://yadav-sachin.github.io/blog/2022/03/01/gre-prep-week-1.html",
            "relUrl": "/2022/03/01/gre-prep-week-1.html",
            "date": " ‚Ä¢ Mar 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Try Plotly on fastpages",
            "content": "Why Plotly . First the plots made in plotly look beautiful as compared to the plots generated in matplotlib and seaborn. (Just look at the 3d plot generated in the examples below ü§© and you would be in love with Plotly too) | Second, the plots are interactive. | . Ok, So what&#39;s the trick? . The trick is simple. Instead of using simple fig.show(), use HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)). . If you use fig.show(), then the figure would not be rendered in fastpages. So we convert the figure to HTML and display the HTML format in notebook. Also, the output of HTML would not be rendered in Jupyter notebook, when you working. So I generally use fig.show() when I am working with the notebook and once done, I replace all fig.show() with the HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)). . Examples . All of the examples below are taken from Plotly Examples gallery . from IPython.display import HTML import plotly.graph_objects as go import plotly.express as px from plotly.subplots import make_subplots # Other Common imports import pandas as pd import numpy as np import re . 3D surface plot with Contour lines . z_data = pd.read_csv(&#39;https://raw.githubusercontent.com/plotly/datasets/master/api_docs/mt_bruno_elevation.csv&#39;) fig = go.Figure(data=[go.Surface(z=z_data.values)]) fig.update_traces(contours_z=dict(show=True, usecolormap=True, highlightcolor=&quot;limegreen&quot;, project_z=True)) fig.update_layout(title=&#39;Mt Bruno Elevation&#39;, autosize=False, scene_camera_eye=dict(x=1.87, y=0.88, z=-0.64), width=500, height=500, margin=dict(l=65, r=50, b=65, t=90) ) # fig.show() HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . 3D scatter plots . df = px.data.iris() fig = px.scatter_3d(df, x=&#39;sepal_length&#39;, y=&#39;sepal_width&#39;, z=&#39;petal_width&#39;, color=&#39;species&#39;) # fig.show() HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . 3D suplots . # Initialize figure with 4 3D subplots fig = make_subplots( rows=2, cols=2, specs=[[{&#39;type&#39;: &#39;surface&#39;}, {&#39;type&#39;: &#39;surface&#39;}], [{&#39;type&#39;: &#39;surface&#39;}, {&#39;type&#39;: &#39;surface&#39;}]]) # Generate data x = np.linspace(-5, 80, 10) y = np.linspace(-5, 60, 10) xGrid, yGrid = np.meshgrid(y, x) z = xGrid ** 3 + yGrid ** 3 # adding surfaces to subplots. fig.add_trace( go.Surface(x=x, y=y, z=z, colorscale=&#39;Viridis&#39;, showscale=False), row=1, col=1) fig.add_trace( go.Surface(x=x, y=y, z=z, colorscale=&#39;RdBu&#39;, showscale=False), row=1, col=2) fig.add_trace( go.Surface(x=x, y=y, z=z, colorscale=&#39;YlOrRd&#39;, showscale=False), row=2, col=1) fig.add_trace( go.Surface(x=x, y=y, z=z, colorscale=&#39;YlGnBu&#39;, showscale=False), row=2, col=2) fig.update_layout( title_text=&#39;3D subplots with different colorscales&#39;, height=800, width=800 ) # fig.show() HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Plotly Sliders . # Create figure fig = go.Figure() # Add traces, one for each slider step for step in np.arange(0, 5, 0.1): fig.add_trace( go.Scatter( visible=False, line=dict(color=&quot;#00CED1&quot;, width=6), name=&quot;ùúà = &quot; + str(step), x=np.arange(0, 10, 0.01), y=np.sin(step * np.arange(0, 10, 0.01)))) # Make 10th trace visible fig.data[10].visible = True # Create and add slider steps = [] for i in range(len(fig.data)): step = dict( method=&quot;update&quot;, args=[{&quot;visible&quot;: [False] * len(fig.data)}, {&quot;title&quot;: &quot;Slider switched to step: &quot; + str(i)}], # layout attribute ) step[&quot;args&quot;][0][&quot;visible&quot;][i] = True # Toggle i&#39;th trace to &quot;visible&quot; steps.append(step) sliders = [dict( active=10, currentvalue={&quot;prefix&quot;: &quot;Frequency: &quot;}, pad={&quot;t&quot;: 50}, steps=steps )] fig.update_layout( sliders=sliders ) # fig.show() HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Sliders and Buttons in Plotly for Animation . url = &quot;https://raw.githubusercontent.com/plotly/datasets/master/gapminderDataFiveYear.csv&quot; dataset = pd.read_csv(url) years = [&quot;1952&quot;, &quot;1962&quot;, &quot;1967&quot;, &quot;1972&quot;, &quot;1977&quot;, &quot;1982&quot;, &quot;1987&quot;, &quot;1992&quot;, &quot;1997&quot;, &quot;2002&quot;, &quot;2007&quot;] # make list of continents continents = [] for continent in dataset[&quot;continent&quot;]: if continent not in continents: continents.append(continent) # make figure fig_dict = { &quot;data&quot;: [], &quot;layout&quot;: {}, &quot;frames&quot;: [] } # fill in most of layout fig_dict[&quot;layout&quot;][&quot;xaxis&quot;] = {&quot;range&quot;: [30, 85], &quot;title&quot;: &quot;Life Expectancy&quot;} fig_dict[&quot;layout&quot;][&quot;yaxis&quot;] = {&quot;title&quot;: &quot;GDP per Capita&quot;, &quot;type&quot;: &quot;log&quot;} fig_dict[&quot;layout&quot;][&quot;hovermode&quot;] = &quot;closest&quot; fig_dict[&quot;layout&quot;][&quot;updatemenus&quot;] = [ { &quot;buttons&quot;: [ { &quot;args&quot;: [None, {&quot;frame&quot;: {&quot;duration&quot;: 500, &quot;redraw&quot;: False}, &quot;fromcurrent&quot;: True, &quot;transition&quot;: {&quot;duration&quot;: 300, &quot;easing&quot;: &quot;quadratic-in-out&quot;}}], &quot;label&quot;: &quot;Play&quot;, &quot;method&quot;: &quot;animate&quot; }, { &quot;args&quot;: [[None], {&quot;frame&quot;: {&quot;duration&quot;: 0, &quot;redraw&quot;: False}, &quot;mode&quot;: &quot;immediate&quot;, &quot;transition&quot;: {&quot;duration&quot;: 0}}], &quot;label&quot;: &quot;Pause&quot;, &quot;method&quot;: &quot;animate&quot; } ], &quot;direction&quot;: &quot;left&quot;, &quot;pad&quot;: {&quot;r&quot;: 10, &quot;t&quot;: 87}, &quot;showactive&quot;: False, &quot;type&quot;: &quot;buttons&quot;, &quot;x&quot;: 0.1, &quot;xanchor&quot;: &quot;right&quot;, &quot;y&quot;: 0, &quot;yanchor&quot;: &quot;top&quot; } ] sliders_dict = { &quot;active&quot;: 0, &quot;yanchor&quot;: &quot;top&quot;, &quot;xanchor&quot;: &quot;left&quot;, &quot;currentvalue&quot;: { &quot;font&quot;: {&quot;size&quot;: 20}, &quot;prefix&quot;: &quot;Year:&quot;, &quot;visible&quot;: True, &quot;xanchor&quot;: &quot;right&quot; }, &quot;transition&quot;: {&quot;duration&quot;: 300, &quot;easing&quot;: &quot;cubic-in-out&quot;}, &quot;pad&quot;: {&quot;b&quot;: 10, &quot;t&quot;: 50}, &quot;len&quot;: 0.9, &quot;x&quot;: 0.1, &quot;y&quot;: 0, &quot;steps&quot;: [] } # make data year = 1952 for continent in continents: dataset_by_year = dataset[dataset[&quot;year&quot;] == year] dataset_by_year_and_cont = dataset_by_year[ dataset_by_year[&quot;continent&quot;] == continent] data_dict = { &quot;x&quot;: list(dataset_by_year_and_cont[&quot;lifeExp&quot;]), &quot;y&quot;: list(dataset_by_year_and_cont[&quot;gdpPercap&quot;]), &quot;mode&quot;: &quot;markers&quot;, &quot;text&quot;: list(dataset_by_year_and_cont[&quot;country&quot;]), &quot;marker&quot;: { &quot;sizemode&quot;: &quot;area&quot;, &quot;sizeref&quot;: 200000, &quot;size&quot;: list(dataset_by_year_and_cont[&quot;pop&quot;]) }, &quot;name&quot;: continent } fig_dict[&quot;data&quot;].append(data_dict) # make frames for year in years: frame = {&quot;data&quot;: [], &quot;name&quot;: str(year)} for continent in continents: dataset_by_year = dataset[dataset[&quot;year&quot;] == int(year)] dataset_by_year_and_cont = dataset_by_year[ dataset_by_year[&quot;continent&quot;] == continent] data_dict = { &quot;x&quot;: list(dataset_by_year_and_cont[&quot;lifeExp&quot;]), &quot;y&quot;: list(dataset_by_year_and_cont[&quot;gdpPercap&quot;]), &quot;mode&quot;: &quot;markers&quot;, &quot;text&quot;: list(dataset_by_year_and_cont[&quot;country&quot;]), &quot;marker&quot;: { &quot;sizemode&quot;: &quot;area&quot;, &quot;sizeref&quot;: 200000, &quot;size&quot;: list(dataset_by_year_and_cont[&quot;pop&quot;]) }, &quot;name&quot;: continent } frame[&quot;data&quot;].append(data_dict) fig_dict[&quot;frames&quot;].append(frame) slider_step = {&quot;args&quot;: [ [year], {&quot;frame&quot;: {&quot;duration&quot;: 300, &quot;redraw&quot;: False}, &quot;mode&quot;: &quot;immediate&quot;, &quot;transition&quot;: {&quot;duration&quot;: 300}} ], &quot;label&quot;: year, &quot;method&quot;: &quot;animate&quot;} sliders_dict[&quot;steps&quot;].append(slider_step) fig_dict[&quot;layout&quot;][&quot;sliders&quot;] = [sliders_dict] fig = go.Figure(fig_dict) # fig.show() HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Custom Buttons . np.random.seed(1) x0 = np.random.normal(2, 0.4, 400) y0 = np.random.normal(2, 0.4, 400) x1 = np.random.normal(3, 0.6, 600) y1 = np.random.normal(6, 0.4, 400) x2 = np.random.normal(4, 0.2, 200) y2 = np.random.normal(4, 0.4, 200) # Create figure fig = go.Figure() # Add traces fig.add_trace( go.Scatter( x=x0, y=y0, mode=&quot;markers&quot;, marker=dict(color=&quot;DarkOrange&quot;) ) ) fig.add_trace( go.Scatter( x=x1, y=y1, mode=&quot;markers&quot;, marker=dict(color=&quot;Crimson&quot;) ) ) fig.add_trace( go.Scatter( x=x2, y=y2, mode=&quot;markers&quot;, marker=dict(color=&quot;RebeccaPurple&quot;) ) ) # Add buttons that add shapes cluster0 = [dict(type=&quot;circle&quot;, xref=&quot;x&quot;, yref=&quot;y&quot;, x0=min(x0), y0=min(y0), x1=max(x0), y1=max(y0), line=dict(color=&quot;DarkOrange&quot;))] cluster1 = [dict(type=&quot;circle&quot;, xref=&quot;x&quot;, yref=&quot;y&quot;, x0=min(x1), y0=min(y1), x1=max(x1), y1=max(y1), line=dict(color=&quot;Crimson&quot;))] cluster2 = [dict(type=&quot;circle&quot;, xref=&quot;x&quot;, yref=&quot;y&quot;, x0=min(x2), y0=min(y2), x1=max(x2), y1=max(y2), line=dict(color=&quot;RebeccaPurple&quot;))] fig.update_layout( updatemenus=[ dict( type=&quot;buttons&quot;, buttons=[ dict(label=&quot;None&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, []]), dict(label=&quot;Cluster 0&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, cluster0]), dict(label=&quot;Cluster 1&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, cluster1]), dict(label=&quot;Cluster 2&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, cluster2]), dict(label=&quot;All&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, cluster0 + cluster1 + cluster2]) ], ) ] ) # Update remaining layout properties fig.update_layout( title_text=&quot;Highlight Clusters&quot;, showlegend=False, ) # fig.show() HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Dropdowns . # Generate dataset import numpy as np np.random.seed(1) x0 = np.random.normal(2, 0.4, 400) y0 = np.random.normal(2, 0.4, 400) x1 = np.random.normal(3, 0.6, 600) y1 = np.random.normal(6, 0.4, 400) x2 = np.random.normal(4, 0.2, 200) y2 = np.random.normal(4, 0.4, 200) # Create figure fig = go.Figure() # Add traces fig.add_trace( go.Scatter( x=x0, y=y0, mode=&quot;markers&quot;, marker=dict(color=&quot;DarkOrange&quot;) ) ) fig.add_trace( go.Scatter( x=x1, y=y1, mode=&quot;markers&quot;, marker=dict(color=&quot;Crimson&quot;) ) ) fig.add_trace( go.Scatter( x=x2, y=y2, mode=&quot;markers&quot;, marker=dict(color=&quot;RebeccaPurple&quot;) ) ) # Add buttons that add shapes cluster0 = [dict(type=&quot;circle&quot;, xref=&quot;x&quot;, yref=&quot;y&quot;, x0=min(x0), y0=min(y0), x1=max(x0), y1=max(y0), line=dict(color=&quot;DarkOrange&quot;))] cluster1 = [dict(type=&quot;circle&quot;, xref=&quot;x&quot;, yref=&quot;y&quot;, x0=min(x1), y0=min(y1), x1=max(x1), y1=max(y1), line=dict(color=&quot;Crimson&quot;))] cluster2 = [dict(type=&quot;circle&quot;, xref=&quot;x&quot;, yref=&quot;y&quot;, x0=min(x2), y0=min(y2), x1=max(x2), y1=max(y2), line=dict(color=&quot;RebeccaPurple&quot;))] fig.update_layout( updatemenus=[ dict(buttons=list([ dict(label=&quot;None&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, []]), dict(label=&quot;Cluster 0&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, cluster0]), dict(label=&quot;Cluster 1&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, cluster1]), dict(label=&quot;Cluster 2&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, cluster2]), dict(label=&quot;All&quot;, method=&quot;relayout&quot;, args=[&quot;shapes&quot;, cluster0 + cluster1 + cluster2]) ]), ) ] ) # Update remaining layout properties fig.update_layout( title_text=&quot;Highlight Clusters&quot;, showlegend=False, ) # fig.show() HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Range Selectors . # Load data df = pd.read_csv( &quot;https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv&quot;) df.columns = [col.replace(&quot;AAPL.&quot;, &quot;&quot;) for col in df.columns] # Create figure fig = go.Figure() fig.add_trace( go.Scatter(x=list(df.Date), y=list(df.High))) # Set title fig.update_layout( title_text=&quot;Time series with range slider and selectors&quot; ) # Add range slider fig.update_layout( xaxis=dict( rangeselector=dict( buttons=list([ dict(count=1, label=&quot;1m&quot;, step=&quot;month&quot;, stepmode=&quot;backward&quot;), dict(count=6, label=&quot;6m&quot;, step=&quot;month&quot;, stepmode=&quot;backward&quot;), dict(count=1, label=&quot;YTD&quot;, step=&quot;year&quot;, stepmode=&quot;todate&quot;), dict(count=1, label=&quot;1y&quot;, step=&quot;year&quot;, stepmode=&quot;backward&quot;), dict(step=&quot;all&quot;) ]) ), rangeslider=dict( visible=True ), type=&quot;date&quot; ) ) # fig.show() HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Figure Widgets: Click Event . The same trick of to_html works for FigureWidgets as well . This Click doesn&#39;t seem to be working well in the Jupyter notebook as well as fastpages. So need to look into it more. . np.random.seed(1) x = np.random.rand(100) y = np.random.rand(100) f = go.FigureWidget([go.Scatter(x=x, y=y, mode=&#39;markers&#39;)]) scatter = f.data[0] colors = [&#39;#a3a7e4&#39;] * 100 scatter.marker.color = colors scatter.marker.size = [10] * 100 f.layout.hovermode = &#39;closest&#39; # create our callback function def update_point(trace, points, selector): c = list(scatter.marker.color) s = list(scatter.marker.size) for i in points.point_inds: c[i] = &#39;#bae2be&#39; s[i] = 20 with f.batch_update(): scatter.marker.color = c scatter.marker.size = s scatter.on_click(update_point) # f.show() HTML(f.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Table and Chart Subplots . df = pd.read_csv(&quot;https://raw.githubusercontent.com/plotly/datasets/master/Mining-BTC-180.csv&quot;) for i, row in enumerate(df[&quot;Date&quot;]): p = re.compile(&quot; 00:00:00&quot;) datetime = p.split(df[&quot;Date&quot;][i])[0] df.iloc[i, 1] = datetime fig = make_subplots( rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.03, specs=[[{&quot;type&quot;: &quot;table&quot;}], [{&quot;type&quot;: &quot;scatter&quot;}], [{&quot;type&quot;: &quot;scatter&quot;}]] ) fig.add_trace( go.Scatter( x=df[&quot;Date&quot;], y=df[&quot;Mining-revenue-USD&quot;], mode=&quot;lines&quot;, name=&quot;mining revenue&quot; ), row=3, col=1 ) fig.add_trace( go.Scatter( x=df[&quot;Date&quot;], y=df[&quot;Hash-rate&quot;], mode=&quot;lines&quot;, name=&quot;hash-rate-TH/s&quot; ), row=2, col=1 ) fig.add_trace( go.Table( header=dict( values=[&quot;Date&quot;, &quot;Number&lt;br&gt;Transactions&quot;, &quot;Output&lt;br&gt;Volume (BTC)&quot;, &quot;Market&lt;br&gt;Price&quot;, &quot;Hash&lt;br&gt;Rate&quot;, &quot;Cost per&lt;br&gt;trans-USD&quot;, &quot;Mining&lt;br&gt;Revenue-USD&quot;, &quot;Trasaction&lt;br&gt;fees-BTC&quot;], font=dict(size=10), align=&quot;left&quot; ), cells=dict( values=[df[k].tolist() for k in df.columns[1:]], align = &quot;left&quot;) ), row=1, col=1 ) fig.update_layout( height=800, showlegend=False, title_text=&quot;Bitcoin mining stats for 180 days&quot;, ) # fig.show() HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Conclusion . Nice, so most of the features of Plotly that I tested above works well on fastpages. . Time to switch to Plotly for most of my graphs/plots on fatpages blogs. üöÄüòÄ .",
            "url": "https://yadav-sachin.github.io/blog/plotly/jupyter/2022/02/25/try-plotly-in-fastpages.html",
            "relUrl": "/plotly/jupyter/2022/02/25/try-plotly-in-fastpages.html",
            "date": " ‚Ä¢ Feb 25, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Machine Learning Quiz 3 Practice",
            "content": "# Common imports import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns import plotly.express as px import plotly.graph_objects as go # Matplotlib Animation from matplotlib.animation import FuncAnimation from matplotlib import animation from matplotlib import rc rc(&#39;animation&#39;, html=&#39;jshtml&#39;) # Interacive input plots from ipywidgets import interact, interactive, fixed, interact_manual import ipywidgets as widgets . . plt.style.use(&quot;ggplot&quot;) sns.set_style(&#39;darkgrid&#39;) # darkgrid, white grid, dark, white and ticks sns.set_palette(&#39;deep&#39;, 8, color_codes = True) plt.rc(&#39;axes&#39;, titlesize=18) # fontsize of the axes title plt.rc(&#39;axes&#39;, labelsize=14) # fontsize of the x and y labels plt.rc(&#39;xtick&#39;, labelsize=13) # fontsize of the tick labels plt.rc(&#39;ytick&#39;, labelsize=13) # fontsize of the tick labels plt.rc(&#39;legend&#39;, fontsize=13) # legend fontsize plt.rc(&#39;font&#39;, size=13) # controls default text sizes . . Lasso Regression . LASSO := Least absolute shrinkage and selection operator . Least absolute $ rightarrow$ that we will be applying $L_{1}$ penalty. | Shrinkage $ rightarrow$ that we are shrinking the $L_{1}$ norm of $ theta$. | Selection $ rightarrow$ the selected features would be sparse. A lot of $ theta_{i}$ will be zero. (Inference would be faster) | . Popular as it would lead to a sparse solution. . Benefits of Sparse $ theta$ . Inference is faster. (Time complexity of Inference in Linear Regression is $O(N times M)$, where N = Number of samples, M = Number of features) | Redundant features can be eliminated in the process. Inherently help in interpretability. | . $$ theta_{opt} = arg_{ theta} min (Y - X theta)^{T}(Y - X theta) : mathopen|| theta mathclose||_{1} &lt; S$$ Using the KKT condition, we get $$ theta_{opt} = arg_{ theta} min (Y - X theta)^{T}(Y - X theta) + delta^{2} mathopen|| theta mathclose||_{1}$$ The above function is a convex function. (sum of two convex functions) . But the $ mathopen|| theta mathclose||_{1}$ is not differentiable . It is not differentiable at $ theta$ = 0 | Sub-gradient is used when we can&#39;t compute gradient for such cases. . | So we can use a generalised gradient descent for sub-gradients here. . | Also, we will explore coordinate descent algorithm. | . np.random.seed(0) N = 30 x = np.random.random(size = N) y = 4 * x + 7 + np.random.normal(scale = 0.2, size = N) fig, ax = plt.subplots(figsize = (10, 5), tight_layout = True) sns.scatterplot(x = x, y = y, ax = ax, label = &quot;Data Points&quot;, color = &quot;b&quot;) sns.lineplot(x = x, y = 4 * x + 7, label = &quot;True Fit&quot;, color = &quot;r&quot;) ax.set(title = &quot;4x + 7&quot;, xlabel = &quot;x&quot;, ylabel = &quot;y&quot;) plt.show() . . Y = y.reshape(-1, 1) X = np.ones((N, 2)) X[:, 1] = x t0_range = np.concatenate([np.linspace(-2, 7, num=20), np.linspace(7, 10, num=100)]) t1_range = np.concatenate( [np.linspace(-2, 4, num=100), np.linspace(4, 8, num=20), np.linspace(8, 10, num=20)] ) T0, T1 = np.meshgrid(t0_range, t1_range) Z = [] for t1 in t1_range: for t0 in t0_range: theta = np.array([t0, t1]).reshape(-1, 1) z = (Y - X @ theta).T @ (Y - X @ theta) Z += [z] Z = np.array(Z).reshape(T0.shape) optimum_theta_idx = np.unravel_index(np.argmin(Z, axis=None), Z.shape) theta0_norms = [] theta1_norms = [] def plot_lasso_plot(update_idx): for each_ax in ax: each_ax.cla() mu = mu_values[update_idx] Z_constrained = [] for t1_idx, t1 in enumerate(t1_range): for t0_idx, t0 in enumerate(t0_range): theta = np.array([t0, t1]).reshape(-1, 1) z_cons = Z[t1_idx][t0_idx] + mu * np.linalg.norm(theta.reshape(-1), ord=1) Z_constrained += [z_cons] Z_constrained = np.array(Z_constrained).reshape(T0.shape) optimum_theta_cons_idx = np.unravel_index( np.argmin(Z_constrained, axis=None), Z.shape ) ax[1].contourf(T0, T1, Z, levels=50, alpha=0.7) ax[1].scatter( T0[optimum_theta_idx], T1[optimum_theta_idx], marker=&quot;*&quot;, s=200, color=&quot;y&quot;, label=&quot;unconstrained optimum&quot;, ) ax[1].scatter( T0[optimum_theta_cons_idx], T1[optimum_theta_cons_idx], marker=&quot;*&quot;, s=200, color=&quot;b&quot;, label=&quot;lasso regression&quot;, ) optimum_theta_cons = np.array( [T0[optimum_theta_cons_idx], T1[optimum_theta_cons_idx]] ).reshape(-1, 1) global theta0_norms, theta1_norms theta0_norms.append(T0[optimum_theta_cons_idx]) theta1_norms.append(T1[optimum_theta_cons_idx]) lasso_theta_norm = np.sum(np.abs(optimum_theta_cons.reshape(-1))) ax[1].fill( [lasso_theta_norm, 0, -lasso_theta_norm, 0], [0, lasso_theta_norm, 0, -lasso_theta_norm], &quot;g&quot;, alpha=0.5, label=rf&quot;$ theta ; L_{1}$ norm constraint $ leq$ {lasso_theta_norm:.4f}&quot;, ) sns.scatterplot(x=x, y=y, ax=ax[0], label=&quot;Data Points&quot;, color=&quot;b&quot;) sns.lineplot( x=x, y=4 * x + 7, ax=ax[0], label=&quot;True Fit&quot;, color=&quot;r&quot;, linestyle=&quot;-.&quot;, lw = 3, ) sns.lineplot( x=X[:, 1], y=(X @ optimum_theta_cons).reshape(-1), ax=ax[0], label=&quot;Lasso Fit&quot;, color=&quot;g&quot;, lw = 3, ) ax[1].legend() ax[0].legend() ax[1].set( title=&quot;Least Square Objective function&quot;, xlabel=r&quot;$ theta_{0}$&quot;, ylabel=r&quot;$ theta_{1}$&quot;, ) ax[0].set( title=rf&quot;Line Fit $ mu$ = {mu:.3f} :: $ theta$ = {optimum_theta_cons.reshape(-1)[0]:.3f}, {optimum_theta_cons.reshape(-1)[1]:.3f}&quot;, xlabel=r&quot;x&quot;, ylabel=r&quot;y&quot;, ) ax[1].set_xlim(-2, 10) ax[1].set_ylim(-2, 10) sns.lineplot( x=mu_values[: update_idx + 1], y=theta0_norms[:update_idx + 1], ax=ax[2], label=r&quot;$ theta_{0}$&quot;, lw = 3, ) sns.lineplot( x=mu_values[: update_idx + 1], y=theta1_norms[:update_idx + 1], ax=ax[2], label=r&quot;$ theta_{1}$&quot;, lw = 3, ) sns.lineplot( x=mu_values[: update_idx + 1], y=np.abs(np.array(theta0_norms[:update_idx + 1])) + np.abs(np.array(theta1_norms[:update_idx + 1])), ax=ax[2], label=r&quot;$L_{1}$ norm $ theta$&quot;, lw = 3, ) ax[2].set( title=r&quot;Regularisation path of $ theta_{i}$&quot;, xlabel=r&quot;$ mu$&quot;, ylabel=&quot;Coefficient Values&quot;, ) ax[2].set_xlim(np.min(mu_values), np.max(mu_values)) ax[2].set_ylim(0, 12) fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(26, 8), tight_layout=True) ax = ax.ravel() mu_values = np.linspace(0, 100, num=16) anim = FuncAnimation( fig, func=plot_lasso_plot, frames=len(mu_values) - 1, interval=1000 / 3 ) plt.close() anim . . &lt;/input&gt; Once Loop Reflect mywriter = animation.PillowWriter(fps=3) anim.save(&#39;./assets/2022-02-25-machine-learning-quiz3/lasso-contour.gif&#39;,writer=mywriter) . . Why Lasso gives sparsity . Geometric Interpretation: . The contour of L1 norm looks more sharper (pointed to burst the contour plot) and is more likely to intersect the contour plots of Least Squares at one of the axis. . For even higher sparsity, we should pick $L_{p} : ; p &lt; 1$ norm as it would be more pointed (like a needle at the axis). But $L_{p}$ is mathematically harder to solve. . Gradient Descent Interpretation . For ridge regression, the gradient depends on the value of $ theta$, so it goes near to value $0$ but never reaches true $0$. But in lasso regression, the gradient of $ mathopen| theta mathclose|$ is independent of value of $ theta$ and depends of sign only. So it can truly reach zero in less number of iterations. . LASSO inherently does feature selection. | Sets coefficient of less important features to zero. | Sparse, efficient and often more interpretable models. | . Coordinate Descent . Another optimization method. Optimization along single coordinate axis in a step. . We can use it for LASSO regression than sub-gradient generalisation of gradient descent. . Idea: Sometimes it is difficult to minimize for all coordinates together (finding the gradient direction along all dimensions) in one step . Coordinate Descent for Unregularised Regression . $$ sum epsilon_{i}^{2} = sum_{i} (y_{i} - ( theta_{0} times x_{i, 0} + theta_{1} times x_{i, 1} + ldots + theta_{d} times x_{i, d}))^{2}$$ . $$ frac{ partial ( sum epsilon_{i}^{2})}{ partial theta_{j}} = 2 times sum_{i} x_{i, j} times (y_{i} - ( theta_{0} times x_{i, 0} + theta_{1} times x_{i, 1} + ldots + theta_{d} times x_{i, d}))$$ . Setting it to zero, $$ theta_{j} = frac{ rho_{j} }{z_{j}} $$ where, $$ z_{j} = sum_{i} x_{i, j}^{2}$$ $$ rho_{j} = sum_{i} (y_{i} - hat{y_{i}}^{(-j)})$$ correlation of residual with $j^{th}$ column of $X$ where, $$ hat{y_{i}}^{(-j)} = y_{i} - (( theta_{0} times x_{i, 0} + theta_{1} times x_{i, 1} + ldots + theta_{d} times x_{i, d})) - theta_{j} times x_{i, j}) $$ the prediction for $i^{th}$ sample without considering $j^{th}$ feature . Coordinate Descent for Lasso Regression . LASSO objective . $$ Obj = sum_{i} epsilon_{i}^{2} + delta^{2} sum_{i} mathopen|| theta_{i} mathclose|| $$ . $$ frac{ partial Obj}{ partial theta_{j}} = -2 rho_{j} + 2 z_{j} theta_{j} + delta^{2} frac{ partial mathopen|| theta_{j} mathclose||}{ partial theta_{j}}$$ . Setting it to zero, . Case 1: If $ rho_{j} &gt; delta^{2} / 2$ ; $ theta_{j} &gt; 0$ $$ theta_{j} = frac{ rho_{j} - delta^{2}}{z_{j}} $$ Case 2: If $ rho_{j} &lt; - delta^{2} / 2$ ; $ theta_{j} &lt; 0$ $$ theta_{j} = frac{ rho_{j} + delta^{2}}{z_{j}} $$ Case 3: If $- delta^{2} / 2 leq rho_{j} leq delta^{2} / 2$ $$ theta_{j} = 0 $$ . The $ delta^{2} / 2$ is a soft-threshold and also does shrinkage in $|| theta_{j}||$. . Bayesian Machine Learning .",
            "url": "https://yadav-sachin.github.io/blog/mlcourse2022/2022/02/25/machine-learning-quiz3.html",
            "relUrl": "/mlcourse2022/2022/02/25/machine-learning-quiz3.html",
            "date": " ‚Ä¢ Feb 25, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Most Frequent How-Tos",
            "content": "The art of &quot;searching on internet&quot; is very crucial. Many times we face some technical tasks, which require us to frequently visit some blogs and Ask Ubuntu pages. So I thought of compiling some of the most common to-dos that I frequently search on the Internet. . How To Configure SSH Key-Based Authentication on a Remote Server . Saving your time to enter the password for SSH login into the remote machine every day. üòâ . https://www.digitalocean.com/community/tutorials/how-to-configure-ssh-key-based-authentication-on-a-linux-server . How to terminate Ubuntu under WSL . Sometimes WSL occupies a lot of memory and CPU usage. It does not gets killed once no longer used. . Short answer: wsl --shutdown in Windows terminal. (terminates all WSL distributions) . Long answer: https://askubuntu.com/questions/1131122/cant-restart-shutdown-ubuntu-under-wsl . How to enable &#39;blackcellmagic&#39; in VS Code Jupyter notebook? . %load_ext blackcellmagic . To use: . %%black . in code cell&#39;s beginning. .",
            "url": "https://yadav-sachin.github.io/blog/howto/2022/02/24/most-frequent-how-to.html",
            "relUrl": "/howto/2022/02/24/most-frequent-how-to.html",
            "date": " ‚Ä¢ Feb 24, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Intro to Gaussian Processes",
            "content": "import numpy as np import seaborn as sns # matplotlib animation import matplotlib.pyplot as plt from matplotlib import rc from matplotlib.animation import FuncAnimation from matplotlib import animation from scipy.stats import multivariate_normal from tqdm.notebook import tqdm # Plotly import plotly.graph_objects as go . plt.style.use(&quot;ggplot&quot;) rc(&#39;animation&#39;, html=&#39;jshtml&#39;) N = 100 # Number of points in mesh = N * N frn = 30 # Number of frames to process in the animation fps = 8 # Frames per second mywriter = animation.PillowWriter(fps=fps) . sns.set_style(&#39;darkgrid&#39;) # darkgrid, white grid, dark, white and ticks plt.rc(&#39;axes&#39;, titlesize=18) # fontsize of the axes title plt.rc(&#39;axes&#39;, labelsize=14) # fontsize of the x and y labels plt.rc(&#39;xtick&#39;, labelsize=13) # fontsize of the tick labels plt.rc(&#39;ytick&#39;, labelsize=13) # fontsize of the tick labels plt.rc(&#39;legend&#39;, fontsize=13) # legend fontsize plt.rc(&#39;font&#39;, size=13) # controls default text sizes . Bi-variate Gaussian Distribution . We have all seen the single variable Gaussian Distribution. For Gaussian Processes, the next step is to check multi-variate Gaussian Distribution. . For $n$-dimensional Gaussian Distribution, the mean $ mu$ is described a 1D vector of size $n$ and a covariance matrix of size $(n times n)$. . mean = [0.5, 0.3] cov = [[1, 0.2], [0.2, 1]] . start, end = -3, 3 distr = multivariate_normal(mean = mean, cov = cov, seed = 42) x = np.linspace(start, end, num = N) y = np.linspace(start, end, num = N) pdf = np.zeros((len(x), len(y))) X, Y = np.meshgrid(x, y) Z = np.zeros((*X.shape, frn)) fig = go.Figure(data = [go.Surface(x = X, y = Y, z = Z)]) fig.update_layout(title=&#39;Gaussian Bivariate Distiru&#39;) fig.show() . import plotly.graph_objects as go import pandas as pd # Read data from a csv z_data = pd.read_csv(&#39;https://raw.githubusercontent.com/plotly/datasets/master/api_docs/mt_bruno_elevation.csv&#39;) fig = go.Figure(data=[go.Surface(z=z_data.values)]) fig.update_layout(title=&#39;Mt Bruno Elevation&#39;, autosize=False, width=500, height=500, margin=dict(l=65, r=50, b=65, t=90)) fig.show() # pyo. . What happens if we change Mean along $x_{0}$ dimension? . Better check with an animation. . mean_x_values = np.linspace(0, 1, num = frn) for k in range(len(mean_x_values)): mean[0] = mean_x_values[k] distr = multivariate_normal(mean = mean, cov = cov, seed = 42) for i in range(X.shape[0]): for j in range(X.shape[1]): Z[i][j][k] = distr.pdf((X[i][j], Y[i][j])) fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) plot = [ax.plot_surface(x, y, Z[:, :, 0], color=&#39;0.75&#39;, rstride=1, cstride=1)] ax.set_title(&quot;Mean along x_0 {:.4f}&quot;.format(mean_x_values[0])) def change_plot(frame_number, Z, plot): plot[0].remove() plot[0] = ax.plot_surface(X, Y, Z[:, :, frame_number], cmap=&quot;coolwarm&quot;) ax.set_xlabel(&quot;x_0&quot;) ax.set_ylabel(&quot;x_1&quot;) ax.set_title(&quot;Mean along x_0 {:.3f}&quot;.format(mean_x_values[frame_number])) ani = FuncAnimation(fig, change_plot, frn, fargs=(Z, plot), interval=1000 / fps) plt.tight_layout() display(ani) mywriter = animation.PillowWriter(fps=fps) ani.save(&#39;./assets/2022-02-24-intro-gaussian-processes/1_mean_animation.gif&#39;,writer=mywriter) plt.clf() . fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.view_init(elev = 90, azim = 0) ax.set_title(&quot;Mean along x_0 {:.4f}&quot;.format(mean_x_values[0])) plot = [ax.plot_surface(x, y, Z[:, :, 0], color=&#39;0.75&#39;, rstride=1, cstride=1)] ani = FuncAnimation(fig, change_plot, frn, fargs=(Z, plot), interval=1000 / fps) plt.tight_layout() display(ani) ani.save(&#39;task1_mean_animation_above.gif&#39;,writer=mywriter) plt.clf() . Varying variance of x0 from 0.1 to 1 . mean = [0.5, 0.3] variance_x_values = np.linspace(0.1, 1, num = frn) for k in range(len(variance_x_values)): cov[0][0] = variance_x_values[k] distr = multivariate_normal(mean = mean, cov = cov, seed = 42) for i in range(X.shape[0]): for j in range(X.shape[1]): Z[i][j][k] = distr.pdf((X[i][j], Y[i][j])) fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.set_xlabel(&quot;x_0&quot;) ax.set_xlabel(&quot;x_1&quot;) plot = [ax.plot_surface(x, y, Z[:, :, 0], color=&#39;0.75&#39;, rstride=1, cstride=1)] ax.set_title(&quot;Variance along x_0 {:.3f}&quot;.format(variance_x_values[0])) def change_plot(frame_number, Z, plot): plot[0].remove() plot[0] = ax.plot_surface(X, Y, Z[:, :, frame_number], cmap=&quot;coolwarm&quot;) ax.set_xlabel(&quot;x_0&quot;) ax.set_ylabel(&quot;x_1&quot;) ax.set_title(&quot;Variance along x_0 {:.3f}&quot;.format(variance_x_values[frame_number])) ax.set_zlim(0, 0.5) ani = FuncAnimation(fig, change_plot, frn, fargs=(Z, plot), interval=1000 / fps) ani.save(&#39;task1_variance_animation.gif&#39;,writer=mywriter) display(ani) plt.clf() . fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.view_init(elev = 90, azim = 0) ax.set_title(&quot;Variance along x_0 {:.3f}&quot;.format(variance_x_values[0])) plot = [ax.plot_surface(x, y, Z[:, :, 0], color=&#39;0.75&#39;, rstride=1, cstride=1)] ax.set_zlim(0, 0.5) ani = FuncAnimation(fig, change_plot, frn, fargs=(Z, plot), interval=1000 / fps) plt.tight_layout() ani.save(&#39;task1_variance_animation_above.gif&#39;,writer=mywriter) display(ani) plt.clf() . Varying covariance from 0 to 1 . mean = [0.5, 0.3] cov = [[1, 0.2], [0.2, 1]] covariance_x_values = np.linspace(-0.99, 0.99, num = frn) for k in range(len(covariance_x_values)): mean[0] = 0.5 mean[1] = 0.3 cov[0][0] = cov[1][1] = 1 cov[0][1] = cov[1][0] = covariance_x_values[k] distr = multivariate_normal(mean = mean, cov = cov, seed = 42) for i in range(X.shape[0]): for j in range(X.shape[1]): Z[i][j][k] = distr.pdf((X[i][j], Y[i][j])) fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.set_xlabel(&quot;x_0&quot;) ax.set_xlabel(&quot;x_1&quot;) ax.set_title(&quot;Covariance {:.3f}&quot;.format(covariance_x_values[0])) plot = [ax.plot_surface(x, y, Z[:, :, 0], color=&#39;0.75&#39;, rstride=1, cstride=1)] def change_plot(frame_number, Z, plot): plot[0].remove() plot[0] = ax.plot_surface(X, Y, Z[:, :, frame_number], cmap=&quot;coolwarm&quot;) ax.set_xlabel(&quot;x_0&quot;) ax.set_ylabel(&quot;x_1&quot;) ax.set_title(&quot;Covariance {:.3f}&quot;.format(covariance_x_values[frame_number])) ax.set_zlim(0, 1) ani = FuncAnimation(fig, change_plot, frn, fargs=(Z, plot), interval=1000 / fps) ani.save(&#39;task1_covariance_animation.gif&#39;,writer=mywriter) display(ani) plt.clf() . fig = plt.figure() ax = fig.add_subplot(111, projection=&#39;3d&#39;) ax.view_init(elev = 90, azim = 0) ax.set_title(&quot;Covariance {:.3f}&quot;.format(covariance_x_values[0])) plot = [ax.plot_surface(x, y, Z[:, :, 0], color=&#39;0.75&#39;, rstride=1, cstride=1)] ax.set_zlim(0, 1) ani = FuncAnimation(fig, change_plot, frn, fargs=(Z, plot), interval=1000 / fps) plt.tight_layout() display(ani) ani.save(&#39;task1_covariance_animation_above.gif&#39;,writer=mywriter) plt.clf() . Task 2 and 3 . def get_prob(X, mean, inv_cov, det_cov): k = inv_cov.shape[0] X = X.reshape((k, -1)) mean_reshape = mean.reshape((k, -1)) numerator = np.matmul(np.matmul((X - mean_reshape).T, inv_cov), X - mean_reshape) numerator = np.exp(-0.5 * numerator) denominator = np.sqrt(np.power(2 * np.pi, k) * det_cov) return numerator / denominator . def draw_samples(mean, cov, a, b, n_points): points = [] k = cov.shape[0] inv_cov = np.linalg.inv(cov) det_cov = np.linalg.det(cov) for i in tqdm(range(n_points)): while True: pt = (b - a) * np.random.random(size = k) + a pxyz = get_prob(np.array(pt), mean, inv_cov, det_cov) if pxyz &gt;= np.random.random(): points.append(pt) break points = np.array(points) return points . Task 3 (dim = 3) . mean = np.array([0.5, 0.3, 0.8]) cov = np.array([[0.8, 0.2, 0], [0.2, 0.9, 0], [0, 0, 0.6]]) . points = draw_samples(mean, cov, -3, 3, int(1e4)) . empirical_mean = np.mean(points, axis = 0) print(&quot;Empirical Mean&quot;, empirical_mean) print(&quot;Actual Mean&quot;, mean) . empirical_cov = np.zeros_like(cov) for i in range(cov.shape[0]): for j in range(cov.shape[1]): empirical_cov[i][j] = np.mean((points[:, i] - empirical_mean[i]) * (points[:, j] - empirical_mean[j])) display(&quot;Empirical Covariance matrix&quot;, empirical_cov) display(&quot;Actual Covariance matrix&quot;, cov) . Task 3 (dim = 5) . mean = np.array([0.5, 0.3, 0.8, 0.1, 0.2]) cov = np.array( [ [0.8, 0.2, 0, 0, 0], [0.2, 0.9, 0, 0, 0], [0, 0, 0.6, 0, 0], [0, 0, 0, 0.9, 0], [0, 0, 0, 0, 0.8]]) . points = draw_samples(mean, cov, -2, 2, int(1e4)) . empirical_mean = np.mean(points, axis = 0) print(&quot;Empirical Mean&quot;, empirical_mean) print(&quot;Actual Mean&quot;, mean) . empirical_cov = np.zeros_like(cov) for i in range(cov.shape[0]): for j in range(cov.shape[1]): empirical_cov[i][j] = np.mean((points[:, i] - empirical_mean[i]) * (points[:, j] - empirical_mean[j])) display(&quot;Empirical Covariance matrix&quot;, empirical_cov) display(&quot;Actual Covariance matrix&quot;, cov) . Box-Muller Method . n_points = int(1e4) points = [(np.random.random(), np.random.random()) for i in range(n_points)] R_func = lambda x, y : np.sqrt(-2 * np.log(x)) theta_func = lambda x, y : 2 * np.pi * y gaussian_points = [ [R_func(x, y) * np.cos(theta_func(x, y)), (R_func(x, y) * np.sin(theta_func(x, y))) ] for (x, y) in points ] gaussian_points = np.array(gaussian_points) . empirical_mean = np.mean(gaussian_points, axis = 0) print(&quot;Empirical Mean&quot;, empirical_mean) empirical_cov = np.zeros((2, 2)) for i in range(empirical_cov.shape[0]): for j in range(empirical_cov.shape[1]): empirical_cov[i][j] = np.mean((gaussian_points[:, i] - empirical_mean[i]) * (gaussian_points[:, j] - empirical_mean[j])) display(&quot;Empirical Covariance matrix&quot;, empirical_cov) . def box_muller_polar_form_standard_normal(size): x = np.empty(size) for idx, _ in np.ndenumerate(x): while True: u, v = 2 * np.random.random() - 1, 2 * np.random.random() - 1 s = u * u + v * v if s == 0 or s &gt;= 1: continue r = np.sqrt((-2 * np.log(s)) / s) x[idx] = r * u break return x . Numpy Method . def numpy_method(mean, cov, size): mean = np.array(mean) cov = np.array(cov).astype(np.double) u, s, v = np.linalg.svd(cov) final_shape = [size, mean.shape[0]] # x = np.random.standard_normal(size = final_shape).reshape((-1, mean.shape[0])) x = box_muller_polar_form_standard_normal(size = final_shape).reshape((-1, mean.shape[0])) x = np.dot(x, np.sqrt(s)[:, None] * v) x += mean return x . samples = numpy_method(mean, cov, int(1e6)) . empirical_mean = samples.mean(axis = 0) print(empirical_mean) . empirical_cov = np.zeros_like(cov) for i in range(cov.shape[0]): for j in range(cov.shape[1]): empirical_cov[i][j] = np.mean((samples[:, i] - empirical_mean[i]) * (samples[:, j] - empirical_mean[j])) display(&quot;Empirical Covariance matrix&quot;, empirical_cov) display(&quot;Actual Covariance matrix&quot;, cov) . Task 4 . dim = 100 mean = np.zeros(dim) points = np.linspace(-5, 5, dim) . Squared Exponential (RBF) Kernel . def square_exponential_kernel(t1, t2, variance, lengthscale): assert variance &gt; 0 assert lengthscale &gt; 0 return variance * np.exp( -1 * np.power(t1 - t2, 2) / (2 * np.power(lengthscale, 2))) . variance, lengthscale = 0.9, 1.22 cov = np.zeros((dim, dim)) for i in range(dim): for j in range(dim): cov[i][j] = square_exponential_kernel(points[i], points[j], variance, lengthscale) . plt.imshow(cov, cmap = &quot;coolwarm&quot;, interpolation=&#39;nearest&#39;) plt.colorbar() . Periodic Kernel . def periodic_kernel(t1, t2, variance, lengthscale, periodicity): assert variance &gt; 0 assert lengthscale &gt; 0 assert periodicity &gt; 0 sine_value = np.sin(np.pi * np.abs(t1 - t2) / periodicity) exp_inner_term = -2 * np.power(sine_value, 2) / np.power(lengthscale, 2) return variance * np.exp(exp_inner_term) . variance, lengthscale, periodicity = 0.88, 0.93, 4 cov = np.zeros((dim, dim)) for i in range(dim): for j in range(dim): cov[i][j] = periodic_kernel(points[i], points[j], variance, lengthscale, periodicity) . plt.imshow(cov, cmap = &quot;coolwarm&quot;, interpolation=&#39;nearest&#39;) plt.colorbar() . Linear Kernel . def linear_kernel(t1, t2, variance, variance_b, offset): assert variance &gt; 0 assert variance_b &gt; 0 return variance_b + variance * (t1 - offset) * (t2 - offset) . variance, variance_b, offset = 0.3, 0.5, 1.0 cov = np.zeros((dim, dim)) for i in range(dim): for j in range(dim): cov[i][j] = linear_kernel(points[i], points[j], variance, variance_b, offset) . plt.imshow(cov, cmap = &quot;coolwarm&quot;, interpolation=&#39;nearest&#39;) plt.colorbar() . Task 5 . dim = 100 mean = np.zeros(dim) points = np.linspace(-5, 5, dim) . RBF Kernel . Change in pathlength from 0 to 5 | . lengthscale_values = np.linspace(1e-3, 5, frn) variance = 0.9 cov = np.zeros((dim, dim, frn)) for k in range(len(lengthscale_values)): lengthscale = lengthscale_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = square_exponential_kernel(points[i], points[j], variance, lengthscale) fig = plt.figure() ax = fig.add_subplot(111) ax.title.set_text(&quot;Change in lengthscale for RBF Kernel&quot;) plot = [ax.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;)] def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax.imshow(cov[:, :, frame_number], cmap=&quot;coolwarm&quot;) ax.title.set_text(&quot;Lengthscale of RBF Kernel {:.3f}&quot;.format(lengthscale_values[frame_number])) plt.colorbar(plot[0]) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) ani.save(&#39;rbf_lengthscale_parameter.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . variance = 0.9 variance_values = np.linspace(1e-3, 5, frn) lengthscale = 1.22 cov = np.zeros((dim, dim, frn)) for k in range(len(variance_values)): variance = variance_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = square_exponential_kernel(points[i], points[j], variance, lengthscale) fig = plt.figure() ax = fig.add_subplot(111) ax.title.set_text(&quot;Change in variance for RBF Kernel&quot;) plot = [ax.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;, vmax = 5)] def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax.imshow(cov[:, :, frame_number], cmap = &quot;coolwarm&quot;, vmax = 5) ax.title.set_text(&quot;Variance in RBF Kernel {:.3f}&quot;.format(variance_values[frame_number])) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) plt.colorbar(plot[0]) ani.save(&#39;rbf_variance_parameter.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . Periodic Kernel . lengthscale_values = np.linspace(1e-3, 5, frn) variance, lengthscale, periodicity = 0.88, 0.93, 4 cov = np.zeros((dim, dim, frn)) for k in range(len(lengthscale_values)): lengthscale = lengthscale_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = periodic_kernel(points[i], points[j], variance, lengthscale, periodicity) fig = plt.figure() ax = fig.add_subplot(111) ax.title.set_text(&quot;Change in lengthscale for Periodic Kernel&quot;) plot = [ax.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;)] def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax.imshow(cov[:, :, frame_number], cmap=&quot;coolwarm&quot;) ax.title.set_text(&quot;Lengthscale in Periodic Kernel {:.3f}&quot;.format(lengthscale_values[frame_number])) plt.colorbar(plot[0]) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) ani.save(&#39;periodic_lengthscale_parameter.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . variance_values = np.linspace(1e-3, 5, frn) variance, lengthscale, periodicity = 0.88, 0.93, 4 cov = np.zeros((dim, dim, frn)) for k in range(len(variance_values)): variance = variance_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = periodic_kernel(points[i], points[j], variance, lengthscale, periodicity) fig = plt.figure() ax = fig.add_subplot(111) ax.title.set_text(&quot;Change in variance for Periodic Kernel&quot;) plot = [ax.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;, vmax = 5)] def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax.imshow(cov[:, :, frame_number], cmap=&quot;coolwarm&quot;, vmax = 5) ax.title.set_text(&quot;Variance in Periodic Kernel {:.3f}&quot;.format(variance_values[frame_number])) plt.colorbar(plot[0]) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) ani.save(&#39;periodic_variance_parameter.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . periodicity_values = np.linspace(1e-3, 5, frn) variance, lengthscale, periodicity = 0.88, 0.93, 4 cov = np.zeros((dim, dim, frn)) for k in range(len(periodicity_values)): periodicity = periodicity_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = periodic_kernel(points[i], points[j], variance, lengthscale, periodicity) fig = plt.figure() ax = fig.add_subplot(111) ax.title.set_text(&quot;Change in periodicity for Periodic Kernel&quot;) plot = [ax.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;)] def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax.imshow(cov[:, :, frame_number], cmap=&quot;coolwarm&quot;) ax.title.set_text(&quot;Periodicity in Periodic Kernel {:.3f}&quot;.format(periodicity_values[frame_number])) plt.colorbar(plot[0]) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) ani.save(&#39;periodic_periodicity_parameter.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . Linear Kernel . variance, variance_b, offset = 0.3, 0.5, 1.0 variance_values = np.linspace(1e-3, 5, frn) cov = np.zeros((dim, dim, frn)) for k in range(len(variance_values)): variance = variance_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = linear_kernel(points[i], points[j], variance, variance_b, offset) fig = plt.figure() ax = fig.add_subplot(111) ax.title.set_text(&quot;Change in variance for Linear Kernel&quot;) plot = [ax.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;, vmax = 20)] def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax.imshow(cov[:, :, frame_number], cmap=&quot;coolwarm&quot;, vmax = 20) ax.title.set_text(&quot;Variance in Linear Kernel {:.3f}&quot;.format(variance_values[frame_number])) plt.colorbar(plot[0]) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) ani.save(&#39;linear_variance_parameter.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . variance, variance_b, offset = 0.3, 0.5, 1.0 variance_b_values = np.linspace(1e-3, 5, frn) cov = np.zeros((dim, dim, frn)) for k in range(len(variance_b_values)): variance_b = variance_b_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = linear_kernel(points[i], points[j], variance, variance_b, offset) fig = plt.figure() ax = fig.add_subplot(111) ax.title.set_text(&quot;Change in variance_b for Linear Kernel&quot;) plot = [ax.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;, vmax = 7)] def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax.imshow(cov[:, :, frame_number], cmap=&quot;coolwarm&quot;, vmax = 7) ax.title.set_text(&quot;variance_b in Linear Kernel {:.3f}&quot;.format(variance_b_values[frame_number])) plt.colorbar(plot[0]) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) ani.save(&#39;linear_variance_b_parameter.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . variance, variance_b, offset = 0.3, 0.5, 1.0 offset_values = np.linspace(1e-3, 5, frn) cov = np.zeros((dim, dim, frn)) for k in range(len(offset_values)): offset = offset_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = linear_kernel(points[i], points[j], variance, variance_b, offset) fig = plt.figure() ax = fig.add_subplot(111) ax.title.set_text(&quot;Change in offset for Linear Kernel&quot;) plot = [ax.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;, vmax = 7)] def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax.imshow(cov[:, :, frame_number], cmap=&quot;coolwarm&quot;, vmax = 7) ax.title.set_text(&quot;offset in Linear Kernel {:.3f}&quot;.format(offset_values[frame_number])) plt.colorbar(plot[0]) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) ani.save(&#39;linear_offset_parameter.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . Task 6 . dim = 50 mean = np.zeros(dim) points = np.linspace(0, dim, num = dim) . fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (50, 10)) variance, lengthscale = 0.9, 1.6 cov = np.zeros((dim, dim)) for i in range(dim): for j in range(i, dim): cov[i][j] = cov[j][i] = square_exponential_kernel(points[i], points[j], variance, lengthscale) print(np.linalg.det(cov)) ax1.imshow(cov, cmap = &quot;coolwarm&quot;) mv = np.random.multivariate_normal(np.zeros(dim), cov, size = 5) xs = np.linspace(1, dim, num = dim) for point in mv: ax2.plot(xs, point, &quot;-&quot;) ax2.scatter(xs, point) . variance_values = np.linspace(1e-3, 10, num = frn) n_samples = 5 variance, lengthscale = 0.9, 1.6 fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 5)) cov = np.zeros((dim, dim, frn)) sampled_points = np.zeros((n_samples, dim, frn)) for k in range(frn): variance = variance_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = square_exponential_kernel(points[i], points[j], variance, lengthscale) sampled_points[:, :, k] = np.random.multivariate_normal(np.zeros(dim), cov[:, :, k], size = n_samples) xs = np.linspace(1, dim - 1, num = dim) plot = [ax1.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;, vmax = 10)] for i in range(1, n_samples + 1): plot.append(ax2.plot(xs, sampled_points[:, :, 0][i - 1])[0]) def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax1.imshow(cov[:, :, frame_number], cmap=&quot;coolwarm&quot;, vmax = 10) for i in range(1, n_samples + 1): plot[i].remove() plot[i] = ax2.plot(xs, sampled_points[:, :, frame_number][i - 1])[0] ax1.title.set_text(&quot;Variance in RBF Kernel {:.3f}&quot;.format(variance_values[frame_number])) ax2.set_ylim(-14, 14) ax2.set_xticks(xs) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) ani.save(&#39;rbf_pattern_variance.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . lengthscale_values = np.linspace(1e-3, 7, num = frn) n_samples = 5 variance, lengthscale = 0.9, 1.6 fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 5)) cov = np.zeros((dim, dim, frn)) sampled_points = np.zeros((n_samples, dim, frn)) for k in range(frn): lengthscale = lengthscale_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = square_exponential_kernel(points[i], points[j], variance, lengthscale) sampled_points[:, :, k] = np.random.multivariate_normal(np.zeros(dim), cov[:, :, k], size = n_samples) xs = np.linspace(0, dim - 1, num = dim) plot = [ax1.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;)] for i in range(1, n_samples + 1): plot.append(ax2.plot(xs, sampled_points[:, :, 0][i - 1])[0]) def change_plot(frame_number, cov, plot): plot[0].remove() plot[0] = ax1.imshow(cov[:, :, frame_number], cmap=&quot;coolwarm&quot;) for i in range(1, n_samples + 1): plot[i].remove() plot[i] = ax2.plot(xs, sampled_points[:, :, frame_number][i - 1])[0] ax1.title.set_text(&quot;Lengthscale in RBF Kernel {:.3f}&quot;.format(lengthscale_values[frame_number])) ax2.set_ylim(-4, 4) ax2.set_xticks(xs) ani = FuncAnimation(fig, change_plot, frn, fargs=(cov, plot), interval=1000 / fps) ani.save(&#39;rbf_pattern_lengthscale.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . Task 7 . def add_train_points(known_points, mean, cov): dim = cov.shape[0] n_known_points = len(known_points) # Known and Unknown indices mean_reshape = mean.reshape((-1, 1)) unknown_indices = [i for i in range(dim)] known_indices = np.array(list(known_points.keys())) for x in known_points.keys(): unknown_indices.remove(x) unknown_indices = np.array(unknown_indices) a = mean_reshape[unknown_indices, :] b = mean_reshape[known_indices, :] A = cov[np.ix_(unknown_indices, unknown_indices)] B = cov[np.ix_(unknown_indices, known_indices)] C = cov[np.ix_(known_indices, known_indices)] y2 = np.array(list(known_points.values())).reshape(-1, 1) inv_C = np.linalg.inv(C) new_mean = np.zeros_like(mean_reshape) new_cov = np.zeros_like(cov) new_mean[known_indices] = y2 new_mean[unknown_indices] = a + B @ inv_C @ (y2 - b) new_cov[unknown_indices[:, None], unknown_indices] = A - B @ inv_C @ B.T return new_mean.reshape(-1), new_cov . dim = 20 cov = np.zeros((dim, dim)) mean = np.zeros(dim) points = np.linspace(0, dim - 1, num = dim) fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (50, 10)) ax2.set_figwidth = 40 variance, lengthscale = 0.9, 1.6 cov = np.zeros((dim, dim)) for i in range(dim): for j in range(i, dim): cov[i][j] = cov[j][i] = square_exponential_kernel(points[i], points[j], variance, lengthscale) print(np.linalg.det(cov)) ax1.imshow(cov, cmap = &quot;coolwarm&quot;) mv = np.random.multivariate_normal(np.zeros(dim), cov, size = 5) xs = np.linspace(0, dim - 1, num = dim) ax2.set_xticks(xs) for point in mv: ax2.scatter(xs, point) ax2.plot(point) . dim = 20 cov = np.zeros((dim, dim)) mean = np.zeros(dim) points = np.linspace(0, dim - 1, num = dim) fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (50, 10)) ax2.set_figwidth = 40 variance, lengthscale = 0.9, 1.6 cov = np.zeros((dim, dim)) for i in range(dim): for j in range(i, dim): cov[i][j] = cov[j][i] = square_exponential_kernel(points[i], points[j], variance, lengthscale) known_points = {5: 2, 6: 3, 10: 2, 16:0.5} new_mean, new_cov = add_train_points(known_points, mean, cov) ax1.imshow(new_cov, cmap = &quot;coolwarm&quot;) mv = np.random.multivariate_normal(new_mean, new_cov, size = 5) xs = np.linspace(0, dim - 1, num = dim) ax2.set_xticks(xs) for point in mv: ax2.scatter(xs, point) ax2.plot(point) . Task 8 . dim = 20 mean = np.zeros(dim) points = np.linspace(0, dim, num = dim) fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (50, 10)) ax2.set_ylim(-10, 10) variance, lengthscale = 0.6, 3 cov = np.zeros((dim, dim)) for i in range(dim): for j in range(i, dim): cov[i][j] = cov[j][i] = square_exponential_kernel(points[i], points[j], variance, lengthscale) print(np.linalg.det(cov)) ax1.imshow(cov, cmap = &quot;coolwarm&quot;) mv = np.random.multivariate_normal(np.zeros(dim), cov, size = 2000) ax2.plot(mean, color=&quot;black&quot;, alpha = 0.5) curve1_ys = [mean[i] + np.sqrt(cov[i][i]) * 1.96 for i in range(dim)] curve2_ys = [mean[i] -np.sqrt(cov[i][i]) * 1.96 for i in range(dim)] xs = np.linspace(0, dim - 1, num = dim) ax2.set_xticks(xs) ax2.scatter(xs, mean, color = &quot;red&quot;) ax2.fill_between(xs, curve1_ys, curve2_ys, color = &quot;gray&quot;, alpha = 0.5) . dim = 20 mean = np.zeros(dim) points = np.linspace(0, dim - 1, num = dim) variance, lengthscale = 0.6, 3 fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (50, 10)) cov = np.zeros((dim, dim)) for i in range(dim): for j in range(i, dim): cov[i][j] = cov[j][i] = square_exponential_kernel(points[i], points[j], variance, lengthscale) print(np.linalg.det(cov)) taken_points = np.random.choice(np.linspace(0, dim - 1, dim), size = int(0.2 * dim)) known_points = dict() for pt in taken_points: known_points[int(pt)] = np.random.random() * 5 print(known_points) new_mean, new_cov = add_train_points(known_points, mean, cov) # new_cov = get_near_psd(new_cov) ax1.imshow(new_cov, cmap = &quot;coolwarm&quot;) print(np.linalg.det(new_cov)) mv = np.random.multivariate_normal(new_mean, new_cov, size = 20000) ax2.plot(new_mean, color=&quot;black&quot;, alpha = 0.5) curve1_ys = [new_mean[i] + np.sqrt(new_cov[i][i]) * 1.96 for i in range(dim)] curve2_ys = [new_mean[i] -np.sqrt(new_cov[i][i]) * 1.96 for i in range(dim)] xs = np.linspace(0, dim - 1, num = dim) ax2.set_xticks(xs) scatter_colors = np.array([(255, 0, 0) for i in range(dim)]) scatter_colors[list(map(int, known_points.keys())), :] = [0, 0, 255] ax2.scatter(xs, new_mean, c = scatter_colors / 255) ax2.fill_between(xs, curve1_ys, curve2_ys, color = &quot;gray&quot;, alpha = 0.5) . Bonus Task . dim = 20 lengthscale_values = np.linspace(1e-3, 5, num = frn) variance, lengthscale = 0.6, 3 fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 5)) known_points = {0:-2, 1:3, 9:-1, 14:-1} cov = np.zeros((dim, dim, frn)) new_cov = np.zeros((dim, dim, frn)) new_mean = np.zeros((dim, frn)) mean = np.zeros(dim) curve1s = np.zeros((dim, frn)) curve2s = np.zeros((dim, frn)) scatter_colors = np.array([(255, 0, 0) for i in range(dim)]) scatter_colors[list(map(int, known_points.keys())), :] = [0, 0, 255] for k in range(frn): lengthscale = lengthscale_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = square_exponential_kernel(points[i], points[j], variance, lengthscale) new_mean[:, k], new_cov[:, :, k] = add_train_points(known_points, mean, cov[:, :, k]) new_cov[:, :, k] = get_near_psd(new_cov[:, :, k]) curve1s[:, k] = [new_mean[:, k][i] + np.sqrt(new_cov[:, :, k][i][i]) * 1.96 for i in range(dim)] curve2s[:, k] = [new_mean[:, k][i] -np.sqrt(new_cov[:, :, k][i][i]) * 1.96 for i in range(dim)] xs = np.linspace(0, dim - 1, num = dim) plot = [ax1.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;), ax2.plot(new_mean[:, 0], color=&quot;black&quot;, alpha = 0.5)[0], ax2.scatter(xs, new_mean[:, 0], c = scatter_colors / 255), ax2.fill_between(xs, curve1s[:, 0], curve2s[:, 0], color = &quot;gray&quot;, alpha = 0.5)] def change_plot(frame_number, new_mean, new_cov, plot): plot[0].remove() plot[0] = ax1.imshow(new_cov[:, :, frame_number], cmap=&quot;coolwarm&quot;) plot[1].remove() plot[1] = ax2.plot(new_mean[:, frame_number], color=&quot;black&quot;, alpha = 0.5)[0] plot[2].remove() plot[2] = ax2.scatter(xs, new_mean[:, frame_number], c = scatter_colors / 255) plot[3].remove() plot[3] = ax2.fill_between(xs, curve1s[:, frame_number], curve2s[:, frame_number], color = &quot;gray&quot;, alpha = 0.5) ax1.title.set_text(&quot;lengthscale in RBF Kernel {:.3f}&quot;.format(lengthscale_values[frame_number])) ax2.set_ylim(-4, 12) ax2.set_xticks(xs) ani = FuncAnimation(fig, change_plot, frn, fargs=(new_mean, new_cov, plot), interval=1000 / fps) ani.save(&#39;bonus_pathlength_pattern.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() . dim = 20 variance_values = np.linspace(1e-3, 5, num = frn) variance, lengthscale = 0.6, 3 fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 5)) known_points = {0:-2, 1:3, 9:-1, 14:-1} cov = np.zeros((dim, dim, frn)) new_cov = np.zeros((dim, dim, frn)) new_mean = np.zeros((dim, frn)) mean = np.zeros(dim) curve1s = np.zeros((dim, frn)) curve2s = np.zeros((dim, frn)) scatter_colors = np.array([(255, 0, 0) for i in range(dim)]) scatter_colors[list(map(int, known_points.keys())), :] = [0, 0, 255] for k in range(frn): variance = variance_values[k] for i in range(dim): for j in range(dim): cov[i][j][k] = square_exponential_kernel(points[i], points[j], variance, lengthscale) new_mean[:, k], new_cov[:, :, k] = add_train_points(known_points, mean, cov[:, :, k]) new_cov[:, :, k] = get_near_psd(new_cov[:, :, k]) curve1s[:, k] = [new_mean[:, k][i] + np.sqrt(new_cov[:, :, k][i][i]) * 1.96 for i in range(dim)] curve2s[:, k] = [new_mean[:, k][i] -np.sqrt(new_cov[:, :, k][i][i]) * 1.96 for i in range(dim)] xs = np.linspace(0, dim - 1, num = dim) plot = [ax1.imshow(cov[:, :, 0], cmap = &quot;coolwarm&quot;), ax2.plot(new_mean[:, 0], color=&quot;black&quot;, alpha = 0.5)[0], ax2.scatter(xs, new_mean[:, 0], c = scatter_colors / 255), ax2.fill_between(xs, curve1s[:, 0], curve2s[:, 0], color = &quot;gray&quot;, alpha = 0.5)] def change_plot(frame_number, new_mean, new_cov, plot): plot[0].remove() plot[0] = ax1.imshow(new_cov[:, :, frame_number], cmap=&quot;coolwarm&quot;, vmax = 4) plot[1].remove() plot[1] = ax2.plot(new_mean[:, frame_number], color=&quot;black&quot;, alpha = 0.5)[0] plot[2].remove() plot[2] = ax2.scatter(xs, new_mean[:, frame_number], c = scatter_colors / 255) plot[3].remove() plot[3] = ax2.fill_between(xs, curve1s[:, frame_number], curve2s[:, frame_number], color = &quot;gray&quot;, alpha = 0.5) ax1.title.set_text(&quot;variance in RBF Kernel {:.3f}&quot;.format(variance_values[frame_number])) ax2.set_ylim(-4, 12) ax2.set_xticks(xs) ani = FuncAnimation(fig, change_plot, frn, fargs=(new_mean, new_cov, plot), interval=1000 / fps) ani.save(&#39;bonus_variance_pattern.gif&#39;,writer=mywriter) plt.tight_layout() display(ani) plt.clf() .",
            "url": "https://yadav-sachin.github.io/blog/fastpages/jupyter/2022/02/24/intro-gaussian-processes.html",
            "relUrl": "/fastpages/jupyter/2022/02/24/intro-gaussian-processes.html",
            "date": " ‚Ä¢ Feb 24, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Most Frequent Python Code Snippets",
            "content": "Matplotlib . Global Settings: Graph Style and Font Size . https://towardsdatascience.com/a-simple-guide-to-beautiful-visualizations-in-python-f564e6b9d392 | ggplot and fivethirtyeight are nice matplotlib styles. | . import matplotlib.pyplot as plt import seaborn as sns . sns.set_style(&#39;darkgrid&#39;) # darkgrid, white grid, dark, white and ticks plt.rc(&#39;axes&#39;, titlesize=18) # fontsize of the axes title plt.rc(&#39;axes&#39;, labelsize=14) # fontsize of the x and y labels plt.rc(&#39;xtick&#39;, labelsize=13) # fontsize of the tick labels plt.rc(&#39;ytick&#39;, labelsize=13) # fontsize of the tick labels plt.rc(&#39;legend&#39;, fontsize=13) # legend fontsize plt.rc(&#39;font&#39;, size=13) # controls default text sizes . plt.style.use(&quot;ggplot&quot;) sns.set_palette(&#39;deep&#39;, 8, color_codes = True) . Subplots, title, axis, legend . fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(12, 5), tight_layout=True) # seaborn ax = sns.histplot(..., palette=&#39;Set2&#39;, linewidth=2) # seaborn will have either the color or palette parameters available (it depends on the plot) #seaborn ax.set(title=&#39;Barplot&#39;, xlabel=&#39;Nationality&#39;, ylabel=&#39;Average Rating&#39;) . Matplotlib Animations . from matplotlib.animation import FuncAnimation from matplotlib import animation from matplotlib import rc rc(&#39;animation&#39;, html=&#39;jshtml&#39;) . anim = FuncAnimation(fig, change_plot, frn, fargs=(Z, plot), interval=1000 / fps) # Close the figure, otherwise would show as duplicate below animation plt.close() display(anim) . mywriter = animation.PillowWriter(fps=fps) anim.save(&#39;gif_name.gif&#39;,writer=mywriter) .",
            "url": "https://yadav-sachin.github.io/blog/python/code/2022/02/24/common-python-snippets.html",
            "relUrl": "/python/code/2022/02/24/common-python-snippets.html",
            "date": " ‚Ä¢ Feb 24, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Active Learning : Maximal Expected Error Reduction",
            "content": "Problem with Expected Model Change . Change in model parameters does not actually correspond to Model performance improvement | . The next logical step we take is to directly focus on Performance metric , rather than to Model Change. So, we will choose the pool instance, which we expect would lead to most model improvement. . Main Idea &#128161; . Choose pool instance which Maximizes &quot;generalization&quot; Error Reduction once we add the instance to training set . We sort of use the pool instances as validation set. . But the pool instances are unlabelled, so we put an estimate to the error and look at expected error reduction. Then we query the instance with minimal expected future error. . import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation from matplotlib import animation from matplotlib import rc import matplotlib from tqdm.notebook import tqdm plt.style.use(&#39;fivethirtyeight&#39;) rc(&#39;animation&#39;, html=&#39;jshtml&#39;) colors = [&#39;purple&#39;,&#39;green&#39;] # To copy the models from copy import deepcopy # Sklearn Imports from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.datasets import make_classification, make_moons from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, f1_score # Progress helper from IPython.display import clear_output . Experiment Configuration . class Config: # Dataset Generation n_features = 2 n_classes = 2 n_samples = 1000 dataset_random_state = 3 noise = 0.2 # Active Learning test_frac = 0.2 init_train_size = 15 model_random_state = 0 active_learning_iterations = 50 # Saved GIF fps = 8 # Base Model n_jobs = 1 . def get_base_model(): # return LogisticRegression(random_state = Config.model_random_state, n_jobs = Config.n_jobs) return SVC(random_state = Config.model_random_state, probability=True) . 2 Class Dataset . X, y = make_classification( n_samples=Config.n_samples, n_features=Config.n_features, n_informative=Config.n_features, n_redundant=0, n_classes=Config.n_classes, random_state=Config.dataset_random_state, shuffle=True, ) plt.figure() plt.scatter(X[:, 0], X[:, 1], c=y, cmap=matplotlib.colors.ListedColormap(colors)) . &lt;matplotlib.collections.PathCollection at 0x7fd1e72f8a90&gt; . Full DataFit . model = get_base_model() model.fit(X, y) . SVC(probability=True, random_state=0) . def plot_decision_surface(X, y, model): grid_X1, grid_X2 = np.meshgrid( np.linspace(X[:, 0].min() - 0.1, X[:, 0].max() + 0.1, 100), np.linspace(X[:, 1].min() - 0.1, X[:, 1].max() + 0.1, 100), ) grid_X = [(x1, x2) for x1, x2 in zip(grid_X1.ravel(), grid_X2.ravel())] grid_pred = model.predict(grid_X) plt.figure(figsize=(6, 5)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=matplotlib.colors.ListedColormap(colors)) plt.contourf(grid_X1, grid_X2, grid_pred.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) . plot_decision_surface(X, y, model) . Train, Pool, Test Split . dataset_indices = list(range(len(X))) train_pool_indices, test_indices = train_test_split(dataset_indices, test_size=Config.test_frac, random_state=0, stratify=y) train_indices, pool_indices = train_test_split(train_pool_indices, train_size=Config.init_train_size, random_state=0) indices_list = [train_indices, pool_indices, test_indices] t_list = [&#39;Train&#39;, &#39;Pool&#39;, &#39;Test&#39;] fig, ax = plt.subplots(1,3,figsize=(15,4), sharex=True, sharey=True) for i in range(3): ax[i].scatter(X[indices_list[i]][:,0], X[indices_list[i]][:,1], c=y[indices_list[i]], cmap=matplotlib.colors.ListedColormap(colors)) ax[i].set_title(t_list[i]) . Fitting Model on Initial Train Data . AL_model = get_base_model() AL_model.fit(X[train_indices], y[train_indices]) . SVC(probability=True, random_state=0) . plot_decision_surface(X[train_indices], y[train_indices], AL_model) . Calculating expected Future Loss . Expected 0/1 Loss is: . . Expected Log-Loss is: . At each iteration: For each unlabeled pool instance: Suppose this instance is labeled and re-train the classifier adding this instance to training set Re-infer the labels of remaining pool instances Estimate the expected error on remaining pool instances - In expected 0/1 loss, we use posterior probability of the most probable class - In expected log loss, we use the posterior class distribution End Select the instance whose expected error reduction is largest . def query_expected_error_reduction(trn_indices, pl_indices, model, loss_type = &quot;01&quot;): y_pred_proba = model.predict_proba(X[pl_indices]) expected_losses = [] # For each pool instance for row_idx, pl_indx in enumerate(pl_indices): new_temp_trn_X = X[trn_indices + [pl_indx]] # Add pool instance to the training instance, assuming this class as the label new_tmp_pl_indices = pl_indices.copy() new_tmp_pl_indices.remove(pl_indx) new_temp_pool_X = X[new_tmp_pl_indices] expec_loss = 0 for clss in range(Config.n_classes): new_temp_trn_y = np.append(y[trn_indices], clss) clss_proba = y_pred_proba[row_idx][clss] # Train the new model new_temp_model = get_base_model() new_temp_model.fit(new_temp_trn_X, new_temp_trn_y) # Re-infer the remaining pool indices new_tmp_y_pred_proba = new_temp_model.predict_proba(new_temp_pool_X) new_tmp_y_pred_log_proba = new_temp_model.predict_log_proba(new_temp_pool_X) new_tmp_y_pred = new_temp_model.predict(new_temp_pool_X) new_tmp_y_pred_class_proba = new_tmp_y_pred_proba[range(len(new_tmp_pl_indices)), new_tmp_y_pred] # Calculate expected loss if loss_type == &quot;01&quot;: loss = np.sum(1 - new_tmp_y_pred_class_proba) elif loss_type == &quot;log&quot;: loss = - np.sum(new_tmp_y_pred_proba * new_tmp_y_pred_log_proba) else: raise ValueError(f&quot;{loss_type} not identified&quot;) expec_loss += clss_proba * loss expected_losses.append(expec_loss) # Select instance with lowest expected error return pl_indices[np.argmin(expected_losses)] . Create test-pool copies for Maximal ERR and Random Strategy . al_train_indices = train_indices.copy() al_test_indices = test_indices.copy() al_pool_indices = pool_indices.copy() random_train_indices = train_indices.copy() random_test_indices = test_indices.copy() random_pool_indices = pool_indices.copy() . Run Active Learning . AL_models = [] random_models = [] AL_added_indices = [] random_added_indices = [] np.random.seed(0) for active_iteration in tqdm(range(Config.active_learning_iterations)): ##### Maximal Expected Error Reduction Strategy ###### # Fit model AL_model = get_base_model() AL_model.fit(X[al_train_indices], y[al_train_indices]) AL_models.append(deepcopy(AL_model)) # Query a point query_idx = query_expected_error_reduction(al_train_indices, al_pool_indices, AL_model, loss_type = &quot;01&quot;) AL_added_indices.append(query_idx) # Add query index to train indices and remove from pool indices al_train_indices.append(query_idx) al_pool_indices.remove(query_idx) ##### Random Strategy ##### # Fit model random_model = get_base_model() random_model.fit(X[random_train_indices], y[random_train_indices]) random_models.append(deepcopy(random_model)) # Query a point query_idx = np.random.choice(random_pool_indices) random_added_indices.append(query_idx) # Add query index to train indices and remove from pool indices random_train_indices.append(query_idx) random_pool_indices.remove(query_idx) . Plot Accuracy . X_test = X[test_indices] y_test = y[test_indices] random_scores = [] AL_scores = [] for iteration in range(Config.active_learning_iterations): clear_output(wait=True) print(&quot;iteration&quot;, iteration) AL_scores.append(accuracy_score(y_test, AL_models[iteration].predict(X_test))) random_scores.append(accuracy_score(y_test, random_models[iteration].predict(X_test))) plt.plot(AL_scores, label=&#39;Active Learning, 0/1 Loss&#39;) plt.plot(random_scores, label=&#39;Random Sampling&#39;) plt.legend() plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Accuracy n(Higher is better)&#39;) . iteration 49 . Text(0, 0.5, &#39;Accuracy n(Higher is better)&#39;) . Plot Decision Boundaries . grid_X1, grid_X2 = np.meshgrid( np.linspace(X[:, 0].min() - 0.1, X[:, 0].max() + 0.1, 100), np.linspace(X[:, 1].min() - 0.1, X[:, 1].max() + 0.1, 100), ) grid_X = [(x1, x2) for x1, x2 in zip(grid_X1.ravel(), grid_X2.ravel())] . X_train, y_train = X[al_train_indices], y[al_train_indices] X_train_rand, y_train_rand = X[random_train_indices], y[random_train_indices] . def update(i): for each in ax: each.cla() AL_grid_preds = AL_models[i].predict(grid_X) random_grid_preds = random_models[i].predict(grid_X) # Active learning ax[0].scatter(X_train[:n_train,0], X_train[:n_train,1], c=y_train[:n_train], label=&#39;initial_train&#39;, alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[0].scatter(X_train[n_train:n_train+i, 0], X_train[n_train:n_train+i, 1], c=y_train[n_train:n_train+i], label=&#39;new_points&#39;, cmap=matplotlib.colors.ListedColormap(colors)) ax[0].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[0].set_title(&#39;New points&#39;) ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label=&#39;test_set&#39;, cmap=matplotlib.colors.ListedColormap(colors)) ax[1].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[1].set_title(&#39;Test points&#39;) ax[0].text(locs[0],locs[1],&#39;Active Learning&#39;) # Random sampling ax[2].scatter(X_train_rand[:n_train,0], X_train_rand[:n_train,1], c=y_train_rand[:n_train], label=&#39;initial_train&#39;, alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[2].scatter(X_train_rand[n_train:n_train+i, 0], X_train_rand[n_train:n_train+i, 1], c=y_train_rand[n_train:n_train+i], label=&#39;new_points&#39;, cmap=matplotlib.colors.ListedColormap(colors)) ax[2].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[2].set_title(&#39;New points&#39;) ax[3].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label=&#39;test_set&#39;, cmap=matplotlib.colors.ListedColormap(colors)) ax[3].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[3].set_title(&#39;Test points&#39;) ax[2].text(locs[0],locs[1],&#39;Random Sampling&#39;) . locs = (2.7, 4) fig, ax = plt.subplots(2,2,figsize=(12,6), sharex=True, sharey=True) ax = ax.ravel() n_train = X_train.shape[0] - Config.active_learning_iterations anim = FuncAnimation(fig, func=update, frames=range(Config.active_learning_iterations)) plt.close() anim . &lt;/input&gt; Once Loop Reflect mywriter = animation.PillowWriter(fps=Config.fps) anim.save(&#39;./assets/2022-02-22-maximal-expected-error-reduction-active-learning/active-learning.gif&#39;,writer=mywriter) . Major Drawbacks: . Very expensive: In each active learning iteration, we have to incrementally re-train the model by adding a new pool instance. Therefore in each iterations, we re-train the model $n _classes times n _pool$ times, which is very expensive. | . The computational cost is linear to the number of classes and number of pool instances. . In most cases, it is the most computationally expensive query framework. . References: . Active Learning Literature Survey by Burr Settles, Section 3.4 | YouTube :: Lecture 26 Active Learning for Network Training: Uncertainty Sampling and other approaches by Florian Marquardt | Original Maximal EER paper.Toward Optimal Active Learning through Sampling Estimation of Error Reduction by Roy and McCallum. ICML, 2001. |",
            "url": "https://yadav-sachin.github.io/blog/activelearning/2022/02/22/maximal-expected-error-reduction-active-learning.html",
            "relUrl": "/activelearning/2022/02/22/maximal-expected-error-reduction-active-learning.html",
            "date": " ‚Ä¢ Feb 22, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Machine Learning Quiz 2 Practice",
            "content": "import numpy as np import pandas as pd import torch import matplotlib.pyplot as plt from ipywidgets import interact, interactive, fixed, interact_manual import ipywidgets as widgets . . np.random.seed(0) torch.manual_seed(0) . . Maths for ML . Given a vector $ epsilon$, we can calculate $ sum epsilon_{i}^{2}$ using $ epsilon^{T} epsilon$ | # As per the convention we take epsilon to be a 2D column vector y = np.array([1.3, 2.5, 6.4, 8.1, 9.0]).reshape(-1, 1) y_hat = np.array([1.5, 2.0, 5.9, 8.5, 9.0]).reshape(-1, 1) epsilon = np.abs(y - y_hat) epsilon_square_sum1 = np.sum(epsilon**2) epsilon_square_sum2 = (epsilon.T @ epsilon).item() assert np.allclose(epsilon_square_sum1, epsilon_square_sum2) . . $(AB)^{T} = B^{T}A^{T}$ | A = np.random.randn(50, 10) B = np.random.randn(10, 20) ab_t = (A @ B).T b_t_a_t = B.T @ A.T assert np.allclose(ab_t, b_t_a_t) . . For a scalar $s$, $s = s^{T}$ | Derivative of scalar $s$ with respect to (yes!, I wrote wrt as full here üòÅ) vector $ theta$ $$ theta = begin{bmatrix} theta_{1} theta_{2} vdots theta{n} end{bmatrix}$$ $$ frac{ partial s}{ partial theta} = begin{bmatrix} frac{ partial s}{ partial theta_{1}} frac{ partial s}{ partial theta_{2}} frac{ partial s}{ partial theta_{3}} vdots frac{ partial s}{ partial theta_{n}} end{bmatrix} $$ | If $A$ is a matrix and $ theta$ is a vector, and $A theta$ is a scalar. Then $$ frac{ partial A theta}{ partial theta} = A^{T} $$ | ü§î Taking some similarity with $a theta$, where both $a$ and $ theta$ are scalar, I have an idea that it would be A. But shape of gradient would be $N times 1$, so $A^{T}$ is my guess before starting any calculations. . N = 20 # as A $ theta$ is scalar, so A.shape[0] should be 1. A = torch.randn((1, N)) theta = torch.randn((N, 1), requires_grad=True) scalar = A @ theta scalar.backward() assert torch.allclose(theta.grad, A.T) . . Assume $Z$ is a matrix of form $X^{T}X$, then $$ frac{ partial ( theta^{T}Z theta)}{ partial theta} = 2Z^{T} theta$$ | ü§î Let me again make a good guess before any calculation, if $ theta$ and $Z$ are both scaler, then the derivative would look like $2Z theta$. So my guess would $2Z theta$, which is equal to $2Z^{T} theta$ as both are $Z$ is symmetric. . X = torch.randn((N, N)) Z = X.T @ X theta = torch.randn((N, 1), requires_grad=True) scalar = theta.T @ Z @ theta scalar.backward() assert torch.allclose(theta.grad, 2 * Z.T @ theta) . . Let&#39;s skip over the content of Rank topic for now. . The maximum rank possible for a matrix is $max(R, C)$ . But an interesting question would be ü§î, what is the minimum rank possible for a matrix, is it 0, is it 1? . Ans: Rank is zero, in case of zero matrix. . Just a leaving thought, if I would have been a developer of Numpy, I would not have allowed np.eye as the method for identity matrix. Better to use np.identity only. üòû . Introduction to Linear Regression . Considering weight as a linear function of height: . $weight_{1} approx theta_{0} + theta_{1} * height_{1}$ | $weight_{2} approx theta_{0} + theta_{1} * height_{2}$ | $weight_{N} approx theta_{0} + theta_{1} * height_{N}$ | . Add extra columns of $1s$ for the bias term in $ theta$ . $$ W_{N times1} = X_{N times2} , theta_{2 times1} $$ where the feature matrix $X$, $X = begin{bmatrix} 1 &amp; height_{1} 1 &amp; height_{2} vdots &amp; vdots 1 &amp; height_{N} end{bmatrix}$ . $ theta_{0}$, Bias/Intercept term : (the value of $y$, when $x$ is set to zero) | $ theta_{1}$, Slope term : (the increase in $y$, when $x$ is increased by 1 unit) | . Generalized Linear Regression . $N$: Number of training samples | $M$: Number of features | . $$ begin{bmatrix} hat{y}_{1} hat{y}_{2} vdots hat{y}_{N} end{bmatrix} _{N times 1} = begin{bmatrix} 1 &amp; x_{1, 1} &amp; x_{1, 2} &amp; ldots &amp; x_{1, M} 1 &amp; x_{2, 1} &amp; x_{2, 2} &amp; ldots &amp; x_{2, M} vdots &amp; vdots &amp; vdots &amp; ldots &amp; vdots 1 &amp; x_{N, 1} &amp; x_{N, 2} &amp; ldots &amp; x_{N, M} end{bmatrix} _{N times (M + 1)} begin{bmatrix} theta_{0} theta_{1} vdots theta_{M} end{bmatrix} _{(M + 1) times 1} $$ $$ hat{y} = X theta $$ . Now, the task at our hand is to estimate &quot;good&quot; values of $ theta$, which will give &quot;good&quot; approximation to the actual values.But how do we decide if a set of values of $ theta$ is &quot;better&quot; than another value of $ theta$. We need a metric for evalution here. . Let $ epsilon_{i}$ be $y_{i} - hat{y}_{i}$, where $ epsilon_{i} sim mathcal{N} (0, sigma^{2})$. We are assuming that $ epsilon_{i}$ is coming from this normal distribution. . We want $| epsilon_{1}|$, $| epsilon_{2}|$, $| epsilon_{3}|$ ... , $| epsilon_{N}|$ to be small. . So we can try to minimize L2 norm (Squared Error) or L1 norm. . weight_height_df = pd.read_csv( &quot;https://raw.githubusercontent.com/yadav-sachin/blog/master/_notebooks/assets/2022-02-17-machine-learning-quiz2-practice/weight-height.csv&quot; ) # take 30 points sampled_idx = np.random.choice(np.arange(len(weight_height_df)), size=30, replace=False) weight_height_df = weight_height_df.iloc[sampled_idx][[&quot;Height&quot;, &quot;Weight&quot;]].sort_values( by=[&quot;Height&quot;] ) def plot_func(theta0, theta1): x = weight_height_df[&quot;Height&quot;] y = weight_height_df[&quot;Weight&quot;] y_hat = theta0 + x * theta1 fig, ax = plt.subplots(figsize = (10, 8)) ax.scatter(x, y, label=&quot;Actual&quot;) ax.plot(x, y_hat, label=&quot;Pred&quot;, linestyle = &quot;--&quot;) ax.legend() ax = plt.gca() ax.set_ylim([50, 400]) mse_val = np.mean((y - y_hat)**2) ax.set_title(rf&quot;$ theta_{0}$={theta0}, $ theta_{1}$={theta1} MSE val: {mse_val:.3f}&quot;) interact( plot_func, theta0=widgets.FloatSlider(name = &quot;theta0 (bias)&quot;, value=-300, min=-1000, max=1000, step=1), theta1=widgets.FloatSlider(name = &quot;theta1 (slope)&quot;, value=7.5, min=-20, max=20, step=0.01), ) . . &lt;function __main__.plot_func(theta0, theta1)&gt; . . Note: Run the notebook in Colab to view the interactive plot above, where we manually change parameters (using sliders) and fit the line through training points with Mean Squared error as the guiding value. . Normal Equation . $$ y = X theta + epsilon$$ Objective: To minimize $ epsilon^{T} epsilon$ $$ epsilon^{T} epsilon = y y^{T} - 2 y^{T}X theta + theta^{T}X^{T}X theta$$ $$ frac{ partial ( epsilon^{T} epsilon)}{ partial theta} = -2X^{T}y + 2X^{T}X theta$$ (we use some of our results from previous chapter &quot;Maths for ML&quot;) . Setting it to zero, $$ theta^{*} = (X^{T}X)^{-1}X^{T}y$$ . Geometric Interpretation of Linear Regression . We have $ hat{y} = X theta$, where $X$ is shape of $(N times M)$, where $M$ is #features and $N$ is #samples. . When we multiply $X$ with column vector $ theta$, the get a column vector which is the linear combination of the columns of matrix $X$. The linear combination, the coeffients of combination are decided by the parameters in $ theta$. So $X theta$ lies in the span of columns of X. . Our objective is to get a $ theta$ to minimize $ mathopen|| y - X theta mathclose||$. . The span of columns of $X$ would be a plane in $N$ dimensional space, $X theta$ lies on this plane. So the least distance, is when the $X theta - y$ is perpendicular to this plane (span of columns of $X$). . Therefore $(y - hat{y}) bot (x_{j}) forall j$ $$X^{T} (y - X theta^{*}) = 0$$ We get, $$ theta^{*} = (X^{T}X)^{-1}X^{T}y$$ . Linear Regression . Relation between #samples and #features in Linear Regression . Let $N$ be number of samples and $M$ be the number of variables/features. . Under-determined system: In Linear Algebra, if we have $N$ equations with $M$ variables and $N &lt; M$, then it is called an under-determined system. In this case, we will have infinite solutions. . Over-determined sytem: When $N &gt; M$, the system is over-determined. In this case the sum of residuals $ sum mathopen| epsilon_{i} mathclose| &gt; 0$, in most cases unless we are able to get perfect fit. . Variable Transformation . In case the output does not seem like a linear combination of the variables, we can also use the higher powers of variables in the linear combination. . We can also use other transformations like logarithm, multiples of more than one variable, etc. And we would still call it &quot;Linear&quot; Regression, as we are here talking about the linearity in coefficients/parameter space ($ theta$). . Multi-collinearity . There can be situations when $X^{T}X$ is a singular matrix. such as $$ X = begin{bmatrix} 1 &amp; 1 &amp; 2 1 &amp; 2 &amp; 4 1 &amp; 3 &amp; 6 end{bmatrix}$$ For $$ begin{bmatrix} x_{1} &amp; x_{2} &amp; y 1 &amp; 2 &amp; 4 2 &amp; 4 &amp; 6 3 &amp; 6 &amp; 8 end{bmatrix} $$ . In this case, a perfect fit is not possible. . In case, we still want to use the normal equation. The ways are: . Regularise: We can add some jitter/noise to the diagonal values and make it invertible. | Drop Variables: As here in this case, $x_{2} = 2 * x_{1}$, so we may drop either one of these variables. | Using different subsets of the data samples might work | Avoid dummy variable trap | . This problem also arises due to dummy variable trap. . Dummy Variables . Let&#39;s assume if we have a categorical variable in linear regression setup, where the air pollution is a function of (#Vehicles, Wind-speed and Wind-direction). As we cannot give categorical values, we need to have corresponding numerical values. . Such as if we have wind-direction, there are 4 categories of this variables namely North, West, East and South. If we go ahead and numerically encode them, North values become $0$, West values become $1$ and so on. . The problem here is that we cannot assign such numerical ordering to the categories, as numerical categories have comparion in case of values. Such as $2$ = $2 times 1$, but is contribution due to West = $2 times$ North . N-1 Variable Encoding: Then to avoid this problem, we use $C-1$ variable encodings, where $C$ is the number of the categories of the categorical variable. The logic is that, to specify the class of the sample, I need to ask at least $C-1$ binary questions. . Is it Class $1$? | Is it Class $2$? | $ vdots$ | Is it Class $C-1$? | . One-hot Variable Encoding: I can ask $C$ binary questions. . Is it Class $1$? | Is it Class $2$? | $ vdots$ | Is it Class $C - 1$? | Is it Class $C$? | . But which one is better? The $C_{th}$ class variable is redundant, as we can determine its values based on $C-1$ class variables. . So One-hot encoding can cause multi-collinearity in Linear Regression, as one of the added variables is redundant and dependent on other variables. . Convexity . Convexity defined over an interval $[ alpha, beta]$, is such that the line segment joining two points $(a, f(a))$, $(b, f(b))$ is above or on the function for all points between in $[a, b]$, given $ alpha leq a, b leq beta$. . One of the problem I noticed is that students/people are just giving out more complex definitions of &quot;convexity&quot;, which makes the things look complex but are actually very simple. So let&#39;s not discuss: other definitions reducing distance, Rolle&#39;s theroem, etc. and keep the definition simple. . Prove $f(x) = x^{2}$ is Convex . Let the two chosen points are $(a, f(a))$ and $(b, f(b))$. . Then any point on the line segment joining the points can be described as $$(t times a + (1 - t) times b, t times f(a) + (1 - t) times f(b))$$ . the corresponding point on the curve at the same $x$-coordinate is $$(t times a + (1 - t) times b, f(t times a + (1 - t) times b))$$ here $ 0 &lt; t &lt; 1$ . The difference in $y$ is: $$t(1-t)(a - b)^{2}$$ which is greater than or equal to zero. Hence the line-segment is always above the function in all points between $a$ and $b$. So function is convex. . Double-Derivative Test . If double-derivative wrt x &gt; 0, then convex. . Double-Derivative Test for multi-parameter function It is equal to Hessian matrix. A function $f(x_{1}, x_{2}, x_{3}, ldots, x_{n})$ is convex iff Hessian Matrix is positive semidefinite for all possible values of $(x_{1}, x_{2}, x_{3}, ldots, x_{n})$. . $$(Hess f)_{ij} equiv frac{ partial^{2} f}{ partial x_{i} partial x_{j} } $$ The Hessian matrix is of shape $(N times N)$ . Let $f(x_{1}, x_{2}) = x_{1}^{2} + x_{2}^{2}$ . $$ Hess(f) = begin{bmatrix} 2 &amp; 0 0 &amp; 2 end{bmatrix} $$ This is positive semidefinite, so $x_{1}^{2} + x_{2}^{2}$ is convex function. . Tip: A quick way to check if a matrix is semi-positive definite is that you take determinant of 1X1, 2X2, ..., NXN upper sub-matrices containing the first pivot point and check if greater than or equal to zero. . Convexity of Least Squares in Linear Regression . $$f( theta) = mathopen|| y - X theta mathclose||^{2}$$ This is equal to $y y^{T} - 2 y^{T}X theta + theta^{T}X^{T}X theta$. . We double differentiate it and get $X^{T}X$, which is a positive semidefinite matrix. . So this is where we connect convexity to Linear Regression. . . If $f(x)$ and $g(x)$ is convex, . $f(x) + g(x)$ is convex. | $kf(x)$ is convex, for $k geq 0$. | . Convex function has this unique property of having a unique minima, which is the global minima. . Gradient Descent . Contour Plots and Gradients . Gradients is the direction for maximum increase in the function value. . To decrease the value of the function the most, we move in the opposite direction of gradient (negative of gradient vector). . Algorithm . Start with some $x_{0}$ | Till convergence or iterations exhausted $x_{i} = x_{i - 1} - alpha frac{ partial f(x)}{ partial x}$, $ alpha$ is the step-size or learning-rate | . | . Quiz Day has arrived . . As very less time remaining to quiz, so moving to important points where need to remember. .",
            "url": "https://yadav-sachin.github.io/blog/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html",
            "relUrl": "/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html",
            "date": " ‚Ä¢ Feb 17, 2022"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Link to my website: https://yadav-sachin.github.io .",
          "url": "https://yadav-sachin.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://yadav-sachin.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}