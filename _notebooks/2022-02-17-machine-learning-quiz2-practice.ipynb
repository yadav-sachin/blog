{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Quiz 2 Practice\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Sachin Yadav\n",
    "- categories: [MLCourse2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maths for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given a vector $\\epsilon$, we can calculate $\\sum\\epsilon_{i}^{2}$ using $\\epsilon^{T} \\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per the convention we take epsilon to be a 2D column vector\n",
    "y = np.array([1.3, 2.5, 6.4, 8.1, 9.0]).reshape(-1, 1)\n",
    "y_hat = np.array([1.5, 2.0, 5.9, 8.5, 9.0]).reshape(-1, 1)\n",
    "\n",
    "epsilon = np.abs(y - y_hat)\n",
    "\n",
    "epsilon_square_sum1 = np.sum(epsilon**2)\n",
    "epsilon_square_sum2 = (epsilon.T @ epsilon).item()\n",
    "\n",
    "assert np.allclose(epsilon_square_sum1, epsilon_square_sum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $(AB)^{T} = B^{T}A^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(50, 10)\n",
    "B = np.random.randn(10, 20)\n",
    "\n",
    "ab_t = (A @ B).T\n",
    "b_t_a_t = B.T @ A.T\n",
    "assert np.allclose(ab_t, b_t_a_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üôÑ Knew it already!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For a scalar $s$, $s = s^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Derivative of scalar $s$ with respect to (yes!, I wrote wrt as full here üòÅ) vector $\\theta$\n",
    "   $$\\theta = \\begin{bmatrix} \\theta_{1} \\\\ \\theta_{2} \\\\ \\vdots \\\\ \\theta{n} \\end{bmatrix}$$\n",
    "    $$\\frac{\\partial s}{\\partial \\theta} = \\begin{bmatrix}\n",
    "     \\frac{\\partial s}{\\partial \\theta_{1}} \\\\\n",
    "     \\frac{\\partial s}{\\partial \\theta_{2}} \\\\\n",
    "     \\frac{\\partial s}{\\partial \\theta_{3}} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\frac{\\partial s}{\\partial \\theta_{n}} \n",
    "     \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. If $A$ is a matrix and $\\theta$ is a vector, and $A\\theta$ is a scalar. Then \n",
    "   $$ \\frac{\\partial A \\theta}{\\partial \\theta} = A^{T} $$\n",
    "\n",
    "ü§î Taking some similarity with $a\\theta$, where both $a$ and $\\theta$ are scalar, I have an idea that it would be A. But shape of gradient would be $N \\times 1$, so $A^{T}$ is my guess before starting any calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "# as A $\\theta$ is scalar, so A.shape[0] should be 1.\n",
    "A = torch.randn((1, N))\n",
    "theta = torch.randn((N, 1), requires_grad=True)\n",
    "scalar = A @ theta\n",
    "scalar.backward()\n",
    "assert torch.allclose(theta.grad, A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç all good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Assume $Z$ is a matrix of form $X^{T}X$, then \n",
    "   $$ \\frac{\\partial (\\theta^{T}Z\\theta)}{\\partial \\theta} = 2Z^{T}\\theta$$\n",
    "\n",
    "ü§î Let me again make a good guess before any calculation, if $\\theta$ and $Z$ are both scaler, then the derivative would look like $2Z\\theta$. So my guess would $2Z\\theta$, which is equal to $2Z^{T}\\theta$ as both are $Z$ is symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn((N, N))\n",
    "Z = X.T @ X\n",
    "theta = torch.randn((N, 1), requires_grad=True)\n",
    "\n",
    "scalar = theta.T @ Z @ theta\n",
    "scalar.backward()\n",
    "\n",
    "assert torch.allclose(theta.grad, 2 * Z.T @ theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëç good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's skip over the content of Rank topic for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum rank possible for a matrix is $max(R, C)$ \n",
    "\n",
    "But an interesting question would be ü§î, what is the minimum rank possible for a matrix, is it 0, is it 1?\n",
    "\n",
    "Ans: Rank is zero, in case of zero matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a leaving thought, if I would have been a developer of Numpy, I would not have allowed `np.eye` as the method for identity matrix. Better to use `np.identity` only. üòû"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d237ae157f8cbaa923dd81e7af592b21f98237404da5d65484215342ef66488a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
