{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Fetching Air Quality OpenAQ data\"\n",
    "> \"Programmatically fetch and parse air quality dataset from OpenAQ\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [openaq, datasets]\n",
    "- image: images/thumbnails/openaq.png\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For accessing the data from the Amazon S3 service, we will use `boto3` and `botocore` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -q boto3 botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for using Amazon S3\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "import multiprocessing\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAQData:\n",
    "    def __init__(\n",
    "        self, download_path, bucket_name=\"openaq-fetches\", prefix=\"realtime-gzipped/\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize class object.\n",
    "        Parameters\n",
    "        ----------\n",
    "        download_path : str\n",
    "            The path for the download directory.\n",
    "        bucket_name : str\n",
    "            The S3 bucket name containng the dataset (the default is \"openaq-fetches\",)\n",
    "        prefix : str\n",
    "            Directory of the required data in the bucket. (the default is \"realtime-gzipped/\")\n",
    "        \"\"\"\n",
    "        self.download_path = download_path\n",
    "        self.bucket_name = bucket_name\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def download_data_date_specific(self, curr_date):\n",
    "        \"\"\"\n",
    "        Downloads air quality data for specific date\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        curr_date : pandas date-time\n",
    "            The date-time for which data needs to be downloaded\n",
    "            From the date-time, the corresponding date is extracted.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.verbose:\n",
    "                print(f\"Downloading: {curr_date}\")\n",
    "            # Extract the date from YYYY-MM-DD HH:MM:SS string\n",
    "            curr_date_str = str(curr_date).split()[0]\n",
    "            data_date_prefix = self.prefix + curr_date_str\n",
    "            s3 = boto3.client(\n",
    "                \"s3\", config=botocore.config.Config(signature_version=botocore.UNSIGNED)\n",
    "            )\n",
    "            curr_date_data_dict = s3.list_objects(\n",
    "                Bucket=self.bucket_name, Prefix=data_date_prefix\n",
    "            )\n",
    "\n",
    "            # If the current date data is not present in the bucket data\n",
    "            if \"Contents\" not in curr_date_data_dict:\n",
    "                print(f\"---------- Unable to fetch Date: {curr_date}----------\")\n",
    "\n",
    "            for file_obj in curr_date_data_dict[\"Contents\"]:\n",
    "                file_obj_name = file_obj[\"Key\"]\n",
    "                file_download_path = os.path.join(self.download_path, file_obj_name)\n",
    "                file_download_dir = os.path.dirname(file_download_path)\n",
    "\n",
    "                if not os.path.exists(file_download_dir):\n",
    "                    os.makedirs(file_download_dir)\n",
    "\n",
    "                s3 = boto3.client(\n",
    "                    \"s3\",\n",
    "                    config=botocore.config.Config(signature_version=botocore.UNSIGNED),\n",
    "                )\n",
    "                s3.download_file(self.bucket_name, file_obj_name, file_download_path)\n",
    "            if self.verbose:\n",
    "                print(f\"Completed: {curr_date}\")\n",
    "        except Exception as err:\n",
    "            print(f\"Error: {err} for {curr_date} \")\n",
    "\n",
    "    def download_data(\n",
    "        self, start_date, end_date, num_multiprocess_pool=5, verbose=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Downloads air quality data from start_data to end_date (both inclusive).\n",
    "        Stores the downloaded data at the self.download_path + \"realtime-gzipped\" directory.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_date : str\n",
    "            Starting date, Format 'yyyy-mm-dd' (inclusive)\n",
    "        end_date : str\n",
    "            Ending date, Format 'yyyy-mm-dd' (inclusive)\n",
    "        verbose : bool, optional\n",
    "            The default is False.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.verbose = verbose\n",
    "        print(f\"---------- Downloading:----------\")\n",
    "        pool = multiprocessing.Pool(num_multiprocess_pool)\n",
    "        n_total_iterations = len(pd.date_range(start=start_date, end=end_date))\n",
    "        for _ in tqdm(\n",
    "            pool.imap(\n",
    "                self.download_data_date_specific,\n",
    "                pd.date_range(start=start_date, end=end_date),\n",
    "            ),\n",
    "            total=n_total_iterations,\n",
    "        ):\n",
    "            pass\n",
    "        pool.close()\n",
    "\n",
    "        print(f\"------------Checking Missing Files ... ----------------\")\n",
    "        (\n",
    "            missing_dates_after_download,\n",
    "            missing_files_after_download,\n",
    "        ) = self.find_missing_data(start_date, end_date)\n",
    "        print(\n",
    "            f\"---------- Missing Dates : {len(missing_dates_after_download)} Dates Missing----------\"\n",
    "        )\n",
    "        if len(missing_dates_after_download):\n",
    "            print(\"Missing Dates are ::\\n\", *missing_dates_after_download)\n",
    "        print(\n",
    "            f\"---------- Missing Files : {len(missing_files_after_download)} files Missing----------\"\n",
    "        )\n",
    "        if len(missing_files_after_download):\n",
    "            print(\"Missing Files are ::\\n\", *missing_files_after_download)\n",
    "\n",
    "    def find_missing_data(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Checks and returns all the missing data filenames in the date range from start_date to end_date (both inclusive).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_date : str\n",
    "            Starting date, Format 'yyyy/mm/dd' (inclusive)\n",
    "        end_date : str\n",
    "            Ending date, Format 'yyyy/mm/dd' (inclusive)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        List (str)\n",
    "            List of the names of the missing files\n",
    "        \"\"\"\n",
    "        dates_missing_list = []\n",
    "        files_missing_list = []\n",
    "        s3 = boto3.client(\n",
    "            \"s3\", config=botocore.config.Config(signature_version=botocore.UNSIGNED)\n",
    "        )\n",
    "        for curr_date in tqdm(pd.date_range(start=start_date, end=end_date)):\n",
    "            curr_date_str = str(curr_date).split()[0]\n",
    "            data_date_prefix = self.prefix + curr_date_str\n",
    "            curr_date_data_dict = s3.list_objects(\n",
    "                Bucket=self.bucket_name, Prefix=data_date_prefix\n",
    "            )\n",
    "\n",
    "            if \"Contents\" not in curr_date_data_dict:\n",
    "                dates_missing_list += [curr_date_str]\n",
    "                continue\n",
    "\n",
    "            for file_obj in curr_date_data_dict[\"Contents\"]:\n",
    "                file_obj_name = file_obj[\"Key\"]\n",
    "                file_download_path = os.path.join(self.download_path, file_obj_name)\n",
    "                if not os.path.exists(file_download_path):\n",
    "                    files_missing_list += [file_obj_name]\n",
    "\n",
    "        return dates_missing_list, files_missing_list\n",
    "\n",
    "    def parse_downloaded_data_folder_wise_required_cities(self, args):\n",
    "        \"\"\"\n",
    "        Parses downloaded data for the specific folder.\n",
    "        Only parses if the data is of date between self.start_date and self.end_date\n",
    "        Filters the data only if city in the self.required_cities list.\n",
    "        The parsed data is stored in \"parsed\" directory in self.download_path\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : arguments\n",
    "            root_path, dirnames, filenames\n",
    "            root_path: The current directory path under consideration\n",
    "            dirnames: List of directory names directly in root_path\n",
    "            filenames: List of filenames directly in root_path\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            root_path, dirnames, filenames = args\n",
    "            if len(filenames) == 0:\n",
    "                return\n",
    "            try:\n",
    "                curr_date = pd.to_datetime(os.path.basename(root_path))\n",
    "            except:\n",
    "                curr_date = pd.to_datetime(\"9999-01-01\")\n",
    "            if curr_date > self.end_date or curr_date < self.start_date:\n",
    "                return\n",
    "            if self.verbose:\n",
    "                print(f\"Parsing: {os.path.basename(root_path)}\")\n",
    "            dataframes_list = []\n",
    "            for filename in filenames:\n",
    "                if \".ndjson.gz\" in filename:\n",
    "                    file_path = os.path.join(root_path, filename)\n",
    "                    json_objs = []\n",
    "                    with gzip.open(file_path, \"rb\") as jl_file:\n",
    "                        for line in jl_file:\n",
    "                            try:\n",
    "                                new_json_obj = json.loads(line)\n",
    "                                json_objs += [new_json_obj]\n",
    "                            except:\n",
    "                                pass\n",
    "                    df = pd.json_normalize(json_objs)\n",
    "                    df = df[\"city\"].isin(self.required_cities)\n",
    "                    dataframes_list.append(df)\n",
    "            combined_df = pd.concat(dataframes_list)\n",
    "            combined_df.to_csv(\n",
    "                os.path.join(\n",
    "                    self.download_path,\n",
    "                    \"parsed\",\n",
    "                    os.path.basename(root_path) + \".csv.gz\",\n",
    "                ),\n",
    "                index=False,\n",
    "                compression=\"gzip\",\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f\"Completed Parsing: {os.path.basename(root_path)}\")\n",
    "        except Exception as err:\n",
    "            print(f\"Error: {err} for {filename} in {root_path}\")\n",
    "\n",
    "    def parse_downloaded_data_required_cities(\n",
    "        self,\n",
    "        start_date,\n",
    "        end_date,\n",
    "        num_multiprocess_pool=5,\n",
    "        required_cities=[\"Delhi\"],\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parses downloaded data.\n",
    "        Only parses if the data is of date between self.start_date and self.end_date (both inclusive)\n",
    "        Filters the data only if city in the self.required_cities list.\n",
    "        The parsed data is stored in \"parsed\" directory in self.download_path\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_date : str\n",
    "            In format 'yyyy/mm/dd'.\n",
    "        end_date : str\n",
    "            In format 'yyyy/mm/dd'\n",
    "        num_multiprocess_pool : int, optional\n",
    "            The number of pool processes in multi-processing. (the default is 5)\n",
    "        required_cities : list[str], optional\n",
    "            The parsed data would contain data for only the cities in required_cities_list. (Case-sensitive) (The default is [\"Delhi\",])\n",
    "        verbose : bool, optional\n",
    "            The default is False.\n",
    "\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.start_date = pd.to_datetime(start_date)\n",
    "        self.end_date = pd.to_datetime(end_date)\n",
    "        self.required_cities = required_cities\n",
    "        self.verbose = verbose\n",
    "        if not os.path.exists(os.path.join(self.download_path, \"parsed\")):\n",
    "            os.makedirs(os.path.join(self.download_path, \"parsed\"))\n",
    "        pool = multiprocessing.Pool(num_multiprocess_pool)\n",
    "        print(f\"---------- Parsing:----------\")\n",
    "        n_total_iterations = len(pd.date_range(start=start_date, end=end_date))\n",
    "        for _ in tqdm(\n",
    "            pool.imap(\n",
    "                self.parse_downloaded_data_folder_wise_required_cities,\n",
    "                os.walk(os.path.join(self.download_path, self.prefix)),\n",
    "            ),\n",
    "            total=n_total_iterations,\n",
    "        ):\n",
    "            pass\n",
    "        pool.close()\n",
    "        print(f\"----------:Parsing Completed----------\")\n",
    "\n",
    "    def find_missing_parsed_files(self, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Checks and returns all the missing parsed data dates in the date range from start_date to end_date (both inclusive).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        start_date : str\n",
    "            Starting date, Format 'yyyy/mm/dd' (inclusive)\n",
    "        end_date : str\n",
    "            Ending date, Format 'yyyy/mm/dd' (inclusive)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        List (str)\n",
    "            List of the dates with missing parsed data\n",
    "        \"\"\"\n",
    "        parsed_files_list = os.listdir(os.path.join(self.download_path, \"parsed\"))\n",
    "        parsed_files_list = [\n",
    "            filename for filename in parsed_files_list if \".csv.gz\" in filename\n",
    "        ]\n",
    "\n",
    "        missing_dates_list = list(pd.date_range(start_date, end_date))\n",
    "\n",
    "        for parsed_filename in parsed_files_list:\n",
    "            file_date = pd.to_datetime(parsed_filename.split(\".\")[0])\n",
    "            if file_date in missing_dates_list:\n",
    "                missing_dates_list.remove(file_date)\n",
    "\n",
    "        missing_dates_list = [\n",
    "            str(date_time).split()[0] for date_time in missing_dates_list\n",
    "        ]\n",
    "        return missing_dates_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with Air Quality data for \"Delhi\" for the month of February 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    ## Download ##\n",
    "\n",
    "    # Data for date range from start_date to end_date (both inclusive)\n",
    "    start_date = \"2021/02/01\"\n",
    "    end_date = \"2021/02/28\"\n",
    "    # Number of simulatenous cores to use in multi-processing\n",
    "    num_multiprocess_pool = 64\n",
    "    # Download directory path\n",
    "    download_path = \"./data/\"\n",
    "    verbose = False\n",
    "\n",
    "    ## Parsing ##\n",
    "\n",
    "    # The cities required in the final parsed data\n",
    "    required_cities = [\n",
    "        \"Delhi\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start with downloading the data.   \n",
    "The dataset would be in form of ndjson files (json lines format)\n",
    "In our code, we are using `multi-processing` library for parallelising the downloading as well as parsing process.\n",
    "\n",
    "\n",
    "Once downloading is complete, a check is run to list all of the missing files, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_aq = OpenAQData(download_path=Config.download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Downloading:----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcab04b75fb4d919a947b730cb8e928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Unable to fetch Date: 2021-02-28 00:00:00----------\n",
      "Error: 'Contents' for 2021-02-28 00:00:00 \n",
      "---------- Unable to fetch Date: 2021-02-20 00:00:00----------\n",
      "Error: 'Contents' for 2021-02-20 00:00:00 \n",
      "------------Checking Missing Files ... ----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3423eda5121d46879d396e3976d0039d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Missing Dates : 2 Dates Missing----------\n",
      "Missing Dates are ::\n",
      " 2021-02-20 2021-02-28\n",
      "---------- Missing Files : 0 files Missing----------\n"
     ]
    }
   ],
   "source": [
    "open_aq.download_data(\n",
    "    start_date=Config.start_date,\n",
    "    end_date=Config.end_date,\n",
    "    num_multiprocess_pool=Config.num_multiprocess_pool,\n",
    "    verbose=Config.verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above cell, we dowloaded data for February 2021.\n",
    "\n",
    "As we can see in the output. For the month of February data is missing for two dates:\n",
    "- 20 February 2022\n",
    "- 28 February 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca78359359e4b90bf7af453db5c0021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "missing_dates_list, missing_files_list = open_aq.find_missing_data(\n",
    "    start_date=Config.start_date, end_date=Config.end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-02-20', '2021-02-28']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_dates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing downloaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single date, the data is distributed in multiple ndjson (json lines) format files.\n",
    "\n",
    "We now parse the overall dataset and store the data is `csv` format.\n",
    "Also as the whole dataset is very large, we will only store results for specific required cities such as Delhi here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Parsing:----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64284f3799f24ab8ab9e2e4e169d55a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------:Parsing Completed----------\n"
     ]
    }
   ],
   "source": [
    "open_aq.parse_downloaded_data_required_cities(\n",
    "    start_date=Config.start_date,\n",
    "    end_date=Config.end_date,\n",
    "    num_multiprocess_pool=Config.num_multiprocess_pool,\n",
    "    required_cities=Config.required_cities,\n",
    "    verbose=Config.verbose,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-02-20', '2021-02-28']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_aq.find_missing_parsed_files(\n",
    "    start_date=Config.start_date, end_date=Config.end_date\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen earlier, after parsing data, the data is currently missing for dates:\n",
    "- 20 February\n",
    "- 28 February \n",
    "for the month of February 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the dataset is now ready for us to do our investigations."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c70e5ee2d2c28a093660d06d1dd4b62023c712f6a8515efc39e65872fc2efeaf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sachin_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
