{
  
    
        "post0": {
            "title": "Active Learning Maximal Expected Error Reduction",
            "content": "Problem with Expected Model Change . Change in model parameters does not actually correspond to Model performance improvement | . The next logical step we take is to directly focus on Performance metric , rather than to Model Change. So, we will choose the pool instance, which we expect would lead to most model improvement. . Main Idea &#128161; . Choose pool instance which Maximizes &quot;generalization&quot; Error Reduction once we add the instance to training set . We sort of use the pool instances as validation set. . But the pool instances are unlabelled, so we put an estimate to the error and look at expected error reduction. Then we query the instance with minimal expected future error. . import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib.animation import FuncAnimation from matplotlib import animation from matplotlib import rc import matplotlib from tqdm.notebook import tqdm plt.style.use(&#39;fivethirtyeight&#39;) rc(&#39;animation&#39;, html=&#39;jshtml&#39;) colors = [&#39;purple&#39;,&#39;green&#39;] # To copy the models from copy import deepcopy # Sklearn Imports from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.datasets import make_classification, make_moons from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, f1_score # Progress helper from IPython.display import clear_output . Experiment Configuration . class Config: # Dataset Generation n_features = 2 n_classes = 2 n_samples = 1000 dataset_random_state = 3 noise = 0.2 # Active Learning test_frac = 0.2 init_train_size = 15 model_random_state = 0 active_learning_iterations = 50 # Saved GIF fps = 8 # Base Model n_jobs = 1 . def get_base_model(): # return LogisticRegression(random_state = Config.model_random_state, n_jobs = Config.n_jobs) return SVC(random_state = Config.model_random_state, probability=True) . 2 Class Dataset . X, y = make_classification( n_samples=Config.n_samples, n_features=Config.n_features, n_informative=Config.n_features, n_redundant=0, n_classes=Config.n_classes, random_state=Config.dataset_random_state, shuffle=True, ) plt.figure() plt.scatter(X[:, 0], X[:, 1], c=y, cmap=matplotlib.colors.ListedColormap(colors)) . &lt;matplotlib.collections.PathCollection at 0x7fd1e72f8a90&gt; . Full DataFit . model = get_base_model() model.fit(X, y) . SVC(probability=True, random_state=0) . def plot_decision_surface(X, y, model): grid_X1, grid_X2 = np.meshgrid( np.linspace(X[:, 0].min() - 0.1, X[:, 0].max() + 0.1, 100), np.linspace(X[:, 1].min() - 0.1, X[:, 1].max() + 0.1, 100), ) grid_X = [(x1, x2) for x1, x2 in zip(grid_X1.ravel(), grid_X2.ravel())] grid_pred = model.predict(grid_X) plt.figure(figsize=(6, 5)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=matplotlib.colors.ListedColormap(colors)) plt.contourf(grid_X1, grid_X2, grid_pred.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) . plot_decision_surface(X, y, model) . Train, Pool, Test Split . dataset_indices = list(range(len(X))) train_pool_indices, test_indices = train_test_split(dataset_indices, test_size=Config.test_frac, random_state=0, stratify=y) train_indices, pool_indices = train_test_split(train_pool_indices, train_size=Config.init_train_size, random_state=0) indices_list = [train_indices, pool_indices, test_indices] t_list = [&#39;Train&#39;, &#39;Pool&#39;, &#39;Test&#39;] fig, ax = plt.subplots(1,3,figsize=(15,4), sharex=True, sharey=True) for i in range(3): ax[i].scatter(X[indices_list[i]][:,0], X[indices_list[i]][:,1], c=y[indices_list[i]], cmap=matplotlib.colors.ListedColormap(colors)) ax[i].set_title(t_list[i]) . Fitting Model on Initial Train Data . AL_model = get_base_model() AL_model.fit(X[train_indices], y[train_indices]) . SVC(probability=True, random_state=0) . plot_decision_surface(X[train_indices], y[train_indices], AL_model) . Expected 0/1 Loss . Calculating expected 0/1 Loss . Expected 0/1 Loss is: . . Expected Log-Loss is: . At each iteration: For each unlabeled pool instance: Suppose this instance is labeled and re-train the classifier adding this instance to training set Re-infer the labels of remaining pool instances Estimate the expected error on remaining pool instances - In expected 0/1 loss, we use posterior probability of the most probable class - In expected log loss, we use the posterior class distribution End Select the instance whose expected error reduction is largest . def query_expected_error_reduction(trn_indices, pl_indices, model, loss_type = &quot;01&quot;): y_pred_proba = model.predict_proba(X[pl_indices]) expected_losses = [] # For each pool instance for row_idx, pl_indx in enumerate(pl_indices): new_temp_trn_X = X[trn_indices + [pl_indx]] # Add pool instance to the training instance, assuming this class as the label new_tmp_pl_indices = pl_indices.copy() new_tmp_pl_indices.remove(pl_indx) new_temp_pool_X = X[new_tmp_pl_indices] expec_loss = 0 for clss in range(Config.n_classes): new_temp_trn_y = np.append(y[trn_indices], clss) clss_proba = y_pred_proba[row_idx][clss] # Train the new model new_temp_model = get_base_model() new_temp_model.fit(new_temp_trn_X, new_temp_trn_y) # Re-infer the remaining pool indices new_tmp_y_pred_proba = new_temp_model.predict_proba(new_temp_pool_X) new_tmp_y_pred_log_proba = new_temp_model.predict_log_proba(new_temp_pool_X) new_tmp_y_pred = new_temp_model.predict(new_temp_pool_X) new_tmp_y_pred_class_proba = new_tmp_y_pred_proba[range(len(new_tmp_pl_indices)), new_tmp_y_pred] # Calculate expected loss if loss_type == &quot;01&quot;: loss = np.sum(1 - new_tmp_y_pred_class_proba) elif loss_type == &quot;log&quot;: loss = - np.sum(new_tmp_y_pred_proba * new_tmp_y_pred_log_proba) else: raise ValueError(f&quot;{loss_type} not identified&quot;) expec_loss += clss_proba * loss expected_losses.append(expec_loss) # Select instance with lowest expected error return pl_indices[np.argmin(expected_losses)] . Create test-pool copies for Maximal ERR and Random Strategy . al_train_indices = train_indices.copy() al_test_indices = test_indices.copy() al_pool_indices = pool_indices.copy() random_train_indices = train_indices.copy() random_test_indices = test_indices.copy() random_pool_indices = pool_indices.copy() . Run Active Learning . AL_models = [] random_models = [] AL_added_indices = [] random_added_indices = [] np.random.seed(0) for active_iteration in tqdm(range(Config.active_learning_iterations)): ##### Maximal Expected Error Reduction Strategy ###### # Fit model AL_model = get_base_model() AL_model.fit(X[al_train_indices], y[al_train_indices]) AL_models.append(deepcopy(AL_model)) # Query a point query_idx = query_expected_error_reduction(al_train_indices, al_pool_indices, AL_model, loss_type = &quot;01&quot;) AL_added_indices.append(query_idx) # Add query index to train indices and remove from pool indices al_train_indices.append(query_idx) al_pool_indices.remove(query_idx) ##### Random Strategy ##### # Fit model random_model = get_base_model() random_model.fit(X[random_train_indices], y[random_train_indices]) random_models.append(deepcopy(random_model)) # Query a point query_idx = np.random.choice(random_pool_indices) random_added_indices.append(query_idx) # Add query index to train indices and remove from pool indices random_train_indices.append(query_idx) random_pool_indices.remove(query_idx) . Plot Accuracy . X_test = X[test_indices] y_test = y[test_indices] random_scores = [] AL_scores = [] for iteration in range(Config.active_learning_iterations): clear_output(wait=True) print(&quot;iteration&quot;, iteration) AL_scores.append(accuracy_score(y_test, AL_models[iteration].predict(X_test))) random_scores.append(accuracy_score(y_test, random_models[iteration].predict(X_test))) plt.plot(AL_scores, label=&#39;Active Learning, 0/1 Loss&#39;) plt.plot(random_scores, label=&#39;Random Sampling&#39;) plt.legend() plt.xlabel(&#39;Iterations&#39;) plt.ylabel(&#39;Accuracy n(Higher is better)&#39;) . iteration 49 . Text(0, 0.5, &#39;Accuracy n(Higher is better)&#39;) . Plot Decision Boundaries . grid_X1, grid_X2 = np.meshgrid( np.linspace(X[:, 0].min() - 0.1, X[:, 0].max() + 0.1, 100), np.linspace(X[:, 1].min() - 0.1, X[:, 1].max() + 0.1, 100), ) grid_X = [(x1, x2) for x1, x2 in zip(grid_X1.ravel(), grid_X2.ravel())] . X_train, y_train = X[al_train_indices], y[al_train_indices] X_train_rand, y_train_rand = X[random_train_indices], y[random_train_indices] . def update(i): for each in ax: each.cla() AL_grid_preds = AL_models[i].predict(grid_X) random_grid_preds = random_models[i].predict(grid_X) # Active learning ax[0].scatter(X_train[:n_train,0], X_train[:n_train,1], c=y_train[:n_train], label=&#39;initial_train&#39;, alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[0].scatter(X_train[n_train:n_train+i, 0], X_train[n_train:n_train+i, 1], c=y_train[n_train:n_train+i], label=&#39;new_points&#39;, cmap=matplotlib.colors.ListedColormap(colors)) ax[0].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[0].set_title(&#39;New points&#39;) ax[1].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label=&#39;test_set&#39;, cmap=matplotlib.colors.ListedColormap(colors)) ax[1].contourf(grid_X1, grid_X2, AL_grid_preds.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[1].set_title(&#39;Test points&#39;) ax[0].text(locs[0],locs[1],&#39;Active Learning&#39;) # Random sampling ax[2].scatter(X_train_rand[:n_train,0], X_train_rand[:n_train,1], c=y_train_rand[:n_train], label=&#39;initial_train&#39;, alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[2].scatter(X_train_rand[n_train:n_train+i, 0], X_train_rand[n_train:n_train+i, 1], c=y_train_rand[n_train:n_train+i], label=&#39;new_points&#39;, cmap=matplotlib.colors.ListedColormap(colors)) ax[2].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[2].set_title(&#39;New points&#39;) ax[3].scatter(X_test[:, 0], X_test[:, 1], c=y_test, label=&#39;test_set&#39;, cmap=matplotlib.colors.ListedColormap(colors)) ax[3].contourf(grid_X1, grid_X2, random_grid_preds.reshape(*grid_X1.shape), alpha=0.2, cmap=matplotlib.colors.ListedColormap(colors)) ax[3].set_title(&#39;Test points&#39;) ax[2].text(locs[0],locs[1],&#39;Random Sampling&#39;) . locs = (2.7, 4) fig, ax = plt.subplots(2,2,figsize=(12,6), sharex=True, sharey=True) ax = ax.ravel() n_train = X_train.shape[0] - Config.active_learning_iterations anim = FuncAnimation(fig, func=update, frames=range(Config.active_learning_iterations)) plt.close() anim . &lt;/input&gt; Once Loop Reflect mywriter = animation.PillowWriter(fps=Config.fps) anim.save(&#39;./assets/2022-02-22-maximal-expected-error-reduction-active-learning/active-learning.gif&#39;,writer=mywriter) . Major Drawbacks: . Very expensive: In each active learning iteration, we have to incrementally re-train the model by adding a new pool instance. Therefore in each iterations, we re-train the model $n _classes times n _pool$ times, which is very expensive. | . The computational cost is linear to the number of classes and number of pool instances. . In most cases, it is the most computationally expensive query framework. . References: . Active Learning Literature Survey by Burr Settles, Section 3.4 | YouTube :: Lecture 26 Active Learning for Network Training: Uncertainty Sampling and other approaches by Florian Marquardt | Original Maximal EER paper.Toward Optimal Active Learning through Sampling Estimation of Error Reduction by Roy and McCallum. ICML, 2001. |",
            "url": "https://yadav-sachin.github.io/blog/activelearning/2022/02/22/maximal-expected-error-reduction-active-learning.html",
            "relUrl": "/activelearning/2022/02/22/maximal-expected-error-reduction-active-learning.html",
            "date": " ‚Ä¢ Feb 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Machine Learning Quiz 2 Practice",
            "content": "import numpy as np import pandas as pd import torch import matplotlib.pyplot as plt from ipywidgets import interact, interactive, fixed, interact_manual import ipywidgets as widgets . . np.random.seed(0) torch.manual_seed(0) . . Maths for ML . Given a vector $ epsilon$, we can calculate $ sum epsilon_{i}^{2}$ using $ epsilon^{T} epsilon$ | # As per the convention we take epsilon to be a 2D column vector y = np.array([1.3, 2.5, 6.4, 8.1, 9.0]).reshape(-1, 1) y_hat = np.array([1.5, 2.0, 5.9, 8.5, 9.0]).reshape(-1, 1) epsilon = np.abs(y - y_hat) epsilon_square_sum1 = np.sum(epsilon**2) epsilon_square_sum2 = (epsilon.T @ epsilon).item() assert np.allclose(epsilon_square_sum1, epsilon_square_sum2) . . $(AB)^{T} = B^{T}A^{T}$ | A = np.random.randn(50, 10) B = np.random.randn(10, 20) ab_t = (A @ B).T b_t_a_t = B.T @ A.T assert np.allclose(ab_t, b_t_a_t) . . For a scalar $s$, $s = s^{T}$ | Derivative of scalar $s$ with respect to (yes!, I wrote wrt as full here üòÅ) vector $ theta$ $$ theta = begin{bmatrix} theta_{1} theta_{2} vdots theta{n} end{bmatrix}$$ $$ frac{ partial s}{ partial theta} = begin{bmatrix} frac{ partial s}{ partial theta_{1}} frac{ partial s}{ partial theta_{2}} frac{ partial s}{ partial theta_{3}} vdots frac{ partial s}{ partial theta_{n}} end{bmatrix} $$ | If $A$ is a matrix and $ theta$ is a vector, and $A theta$ is a scalar. Then $$ frac{ partial A theta}{ partial theta} = A^{T} $$ | ü§î Taking some similarity with $a theta$, where both $a$ and $ theta$ are scalar, I have an idea that it would be A. But shape of gradient would be $N times 1$, so $A^{T}$ is my guess before starting any calculations. . N = 20 # as A $ theta$ is scalar, so A.shape[0] should be 1. A = torch.randn((1, N)) theta = torch.randn((N, 1), requires_grad=True) scalar = A @ theta scalar.backward() assert torch.allclose(theta.grad, A.T) . . Assume $Z$ is a matrix of form $X^{T}X$, then $$ frac{ partial ( theta^{T}Z theta)}{ partial theta} = 2Z^{T} theta$$ | ü§î Let me again make a good guess before any calculation, if $ theta$ and $Z$ are both scaler, then the derivative would look like $2Z theta$. So my guess would $2Z theta$, which is equal to $2Z^{T} theta$ as both are $Z$ is symmetric. . X = torch.randn((N, N)) Z = X.T @ X theta = torch.randn((N, 1), requires_grad=True) scalar = theta.T @ Z @ theta scalar.backward() assert torch.allclose(theta.grad, 2 * Z.T @ theta) . . Let&#39;s skip over the content of Rank topic for now. . The maximum rank possible for a matrix is $max(R, C)$ . But an interesting question would be ü§î, what is the minimum rank possible for a matrix, is it 0, is it 1? . Ans: Rank is zero, in case of zero matrix. . Just a leaving thought, if I would have been a developer of Numpy, I would not have allowed np.eye as the method for identity matrix. Better to use np.identity only. üòû . Introduction to Linear Regression . Considering weight as a linear function of height: . $weight_{1} approx theta_{0} + theta_{1} * height_{1}$ | $weight_{2} approx theta_{0} + theta_{1} * height_{2}$ | $weight_{N} approx theta_{0} + theta_{1} * height_{N}$ | . Add extra columns of $1s$ for the bias term in $ theta$ . $$ W_{N times1} = X_{N times2} , theta_{2 times1} $$ where the feature matrix $X$, $X = begin{bmatrix} 1 &amp; height_{1} 1 &amp; height_{2} vdots &amp; vdots 1 &amp; height_{N} end{bmatrix}$ . $ theta_{0}$, Bias/Intercept term : (the value of $y$, when $x$ is set to zero) | $ theta_{1}$, Slope term : (the increase in $y$, when $x$ is increased by 1 unit) | . Generalized Linear Regression . $N$: Number of training samples | $M$: Number of features | . $$ begin{bmatrix} hat{y}_{1} hat{y}_{2} vdots hat{y}_{N} end{bmatrix} _{N times 1} = begin{bmatrix} 1 &amp; x_{1, 1} &amp; x_{1, 2} &amp; ldots &amp; x_{1, M} 1 &amp; x_{2, 1} &amp; x_{2, 2} &amp; ldots &amp; x_{2, M} vdots &amp; vdots &amp; vdots &amp; ldots &amp; vdots 1 &amp; x_{N, 1} &amp; x_{N, 2} &amp; ldots &amp; x_{N, M} end{bmatrix} _{N times (M + 1)} begin{bmatrix} theta_{0} theta_{1} vdots theta_{M} end{bmatrix} _{(M + 1) times 1} $$ $$ hat{y} = X theta $$ . Now, the task at our hand is to estimate &quot;good&quot; values of $ theta$, which will give &quot;good&quot; approximation to the actual values.But how do we decide if a set of values of $ theta$ is &quot;better&quot; than another value of $ theta$. We need a metric for evalution here. . Let $ epsilon_{i}$ be $y_{i} - hat{y}_{i}$, where $ epsilon_{i} sim mathcal{N} (0, sigma^{2})$. We are assuming that $ epsilon_{i}$ is coming from this normal distribution. . We want $| epsilon_{1}|$, $| epsilon_{2}|$, $| epsilon_{3}|$ ... , $| epsilon_{N}|$ to be small. . So we can try to minimize L2 norm (Squared Error) or L1 norm. . weight_height_df = pd.read_csv( &quot;https://raw.githubusercontent.com/yadav-sachin/blog/master/_notebooks/assets/2022-02-17-machine-learning-quiz2-practice/weight-height.csv&quot; ) # take 30 points sampled_idx = np.random.choice(np.arange(len(weight_height_df)), size=30, replace=False) weight_height_df = weight_height_df.iloc[sampled_idx][[&quot;Height&quot;, &quot;Weight&quot;]].sort_values( by=[&quot;Height&quot;] ) def plot_func(theta0, theta1): x = weight_height_df[&quot;Height&quot;] y = weight_height_df[&quot;Weight&quot;] y_hat = theta0 + x * theta1 fig, ax = plt.subplots(figsize = (10, 8)) ax.scatter(x, y, label=&quot;Actual&quot;) ax.plot(x, y_hat, label=&quot;Pred&quot;, linestyle = &quot;--&quot;) ax.legend() ax = plt.gca() ax.set_ylim([50, 400]) mse_val = np.mean((y - y_hat)**2) ax.set_title(rf&quot;$ theta_{0}$={theta0}, $ theta_{1}$={theta1} MSE val: {mse_val:.3f}&quot;) interact( plot_func, theta0=widgets.FloatSlider(name = &quot;theta0 (bias)&quot;, value=-300, min=-1000, max=1000, step=1), theta1=widgets.FloatSlider(name = &quot;theta1 (slope)&quot;, value=7.5, min=-20, max=20, step=0.01), ) . . &lt;function __main__.plot_func(theta0, theta1)&gt; . . Note: Run the notebook in Colab to view the interactive plot above, where we manually change parameters (using sliders) and fit the line through training points with Mean Squared error as the guiding value. . Normal Equation . $$ y = X theta + epsilon$$ Objective: To minimize $ epsilon^{T} epsilon$ $$ epsilon^{T} epsilon = y y^{T} - 2 y^{T}X theta + theta^{T}X^{T}X theta$$ $$ frac{ partial ( epsilon^{T} epsilon)}{ partial theta} = -2X^{T}y + 2X^{T}X theta$$ (we use some of our results from previous chapter &quot;Maths for ML&quot;) . Setting it to zero, $$ theta^{*} = (X^{T}X)^{-1}X^{T}y$$ . Geometric Interpretation of Linear Regression . We have $ hat{y} = X theta$, where $X$ is shape of $(N times M)$, where $M$ is #features and $N$ is #samples. . When we multiply $X$ with column vector $ theta$, the get a column vector which is the linear combination of the columns of matrix $X$. The linear combination, the coeffients of combination are decided by the parameters in $ theta$. So $X theta$ lies in the span of columns of X. . Our objective is to get a $ theta$ to minimize $ mathopen|| y - X theta mathclose||$. . The span of columns of $X$ would be a plane in $N$ dimensional space, $X theta$ lies on this plane. So the least distance, is when the $X theta - y$ is perpendicular to this plane (span of columns of $X$). . Therefore $(y - hat{y}) bot (x_{j}) forall j$ $$X^{T} (y - X theta^{*}) = 0$$ We get, $$ theta^{*} = (X^{T}X)^{-1}X^{T}y$$ . Linear Regression . Relation between #samples and #features in Linear Regression . Let $N$ be number of samples and $M$ be the number of variables/features. . Under-determined system: In Linear Algebra, if we have $N$ equations with $M$ variables and $N &lt; M$, then it is called an under-determined system. In this case, we will have infinite solutions. . Over-determined sytem: When $N &gt; M$, the system is over-determined. In this case the sum of residuals $ sum mathopen| epsilon_{i} mathclose| &gt; 0$, in most cases unless we are able to get perfect fit. . Variable Transformation . In case the output does not seem like a linear combination of the variables, we can also use the higher powers of variables in the linear combination. . We can also use other transformations like logarithm, multiples of more than one variable, etc. And we would still call it &quot;Linear&quot; Regression, as we are here talking about the linearity in coefficients/parameter space ($ theta$). . Multi-collinearity . There can be situations when $X^{T}X$ is a singular matrix. such as $$ X = begin{bmatrix} 1 &amp; 1 &amp; 2 1 &amp; 2 &amp; 4 1 &amp; 3 &amp; 6 end{bmatrix}$$ For $$ begin{bmatrix} x_{1} &amp; x_{2} &amp; y 1 &amp; 2 &amp; 4 2 &amp; 4 &amp; 6 3 &amp; 6 &amp; 8 end{bmatrix} $$ . In this case, a perfect fit is not possible. . In case, we still want to use the normal equation. The ways are: . Regularise: We can add some jitter/noise to the diagonal values and make it invertible. | Drop Variables: As here in this case, $x_{2} = 2 * x_{1}$, so we may drop either one of these variables. | Using different subsets of the data samples might work | Avoid dummy variable trap | . This problem also arises due to dummy variable trap. . Dummy Variables . Let&#39;s assume if we have a categorical variable in linear regression setup, where the air pollution is a function of (#Vehicles, Wind-speed and Wind-direction). As we cannot give categorical values, we need to have corresponding numerical values. . Such as if we have wind-direction, there are 4 categories of this variables namely North, West, East and South. If we go ahead and numerically encode them, North values become $0$, West values become $1$ and so on. . The problem here is that we cannot assign such numerical ordering to the categories, as numerical categories have comparion in case of values. Such as $2$ = $2 times 1$, but is contribution due to West = $2 times$ North . N-1 Variable Encoding: Then to avoid this problem, we use $C-1$ variable encodings, where $C$ is the number of the categories of the categorical variable. The logic is that, to specify the class of the sample, I need to ask at least $C-1$ binary questions. . Is it Class $1$? | Is it Class $2$? | $ vdots$ | Is it Class $C-1$? | . One-hot Variable Encoding: I can ask $C$ binary questions. . Is it Class $1$? | Is it Class $2$? | $ vdots$ | Is it Class $C - 1$? | Is it Class $C$? | . But which one is better? The $C_{th}$ class variable is redundant, as we can determine its values based on $C-1$ class variables. . So One-hot encoding can cause multi-collinearity in Linear Regression, as one of the added variables is redundant and dependent on other variables. . Convexity . Convexity defined over an interval $[ alpha, beta]$, is such that the line segment joining two points $(a, f(a))$, $(b, f(b))$ is above or on the function for all points between in $[a, b]$, given $ alpha leq a, b leq beta$. . One of the problem I noticed is that students/people are just giving out more complex definitions of &quot;convexity&quot;, which makes the things look complex but are actually very simple. So let&#39;s not discuss: other definitions reducing distance, Rolle&#39;s theroem, etc. and keep the definition simple. . Prove $f(x) = x^{2}$ is Convex . Let the two chosen points are $(a, f(a))$ and $(b, f(b))$. . Then any point on the line segment joining the points can be described as $$(t times a + (1 - t) times b, t times f(a) + (1 - t) times f(b))$$ . the corresponding point on the curve at the same $x$-coordinate is $$(t times a + (1 - t) times b, f(t times a + (1 - t) times b))$$ here $ 0 &lt; t &lt; 1$ . The difference in $y$ is: $$t(1-t)(a - b)^{2}$$ which is greater than or equal to zero. Hence the line-segment is always above the function in all points between $a$ and $b$. So function is convex. . Double-Derivative Test . If double-derivative wrt x &gt; 0, then convex. . Double-Derivative Test for multi-parameter function It is equal to Hessian matrix. A function $f(x_{1}, x_{2}, x_{3}, ldots, x_{n})$ is convex iff Hessian Matrix is positive semidefinite for all possible values of $(x_{1}, x_{2}, x_{3}, ldots, x_{n})$. . $$(Hess f)_{ij} equiv frac{ partial^{2} f}{ partial x_{i} partial x_{j} } $$ The Hessian matrix is of shape $(N times N)$ . Let $f(x_{1}, x_{2}) = x_{1}^{2} + x_{2}^{2}$ . $$ Hess(f) = begin{bmatrix} 2 &amp; 0 0 &amp; 2 end{bmatrix} $$ This is positive semidefinite, so $x_{1}^{2} + x_{2}^{2}$ is convex function. . Tip: A quick way to check if a matrix is semi-positive definite is that you take determinant of 1X1, 2X2, ..., NXN upper sub-matrices containing the first pivot point and check if greater than or equal to zero. . Convexity of Least Squares in Linear Regression . $$f( theta) = mathopen|| y - X theta mathclose||^{2}$$ This is equal to $y y^{T} - 2 y^{T}X theta + theta^{T}X^{T}X theta$. . We double differentiate it and get $X^{T}X$, which is a positive semidefinite matrix. . So this is where we connect convexity to Linear Regression. . . If $f(x)$ and $g(x)$ is convex, . $f(x) + g(x)$ is convex. | $kf(x)$ is convex, for $k geq 0$. | . Convex function has this unique property of having a unique minima, which is the global minima. . Gradient Descent . Contour Plots and Gradients . Gradients is the direction for maximum increase in the function value. . To decrease the value of the function the most, we move in the opposite direction of gradient (negative of gradient vector). . Algorithm . Start with some $x_{0}$ | Till convergence or iterations exhausted $x_{i} = x_{i - 1} - alpha frac{ partial f(x)}{ partial x}$, $ alpha$ is the step-size or learning-rate | . | . Quiz Day has arrived . . As very less time remaining to quiz, so moving to important points where need to remember. .",
            "url": "https://yadav-sachin.github.io/blog/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html",
            "relUrl": "/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html",
            "date": " ‚Ä¢ Feb 17, 2022"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://yadav-sachin.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://yadav-sachin.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}