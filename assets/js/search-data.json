{
  
    
        "post0": {
            "title": "Machine Learning Quiz 2 Practice",
            "content": "import numpy as np import pandas as pd import torch import matplotlib.pyplot as plt from ipywidgets import interact, interactive, fixed, interact_manual import ipywidgets as widgets . . np.random.seed(0) . %pip install -q blackcellmagic %load_ext blackcellmagic . . Note: you may need to restart the kernel to use updated packages. . Maths for ML . Given a vector $ epsilon$, we can calculate $ sum epsilon_{i}^{2}$ using $ epsilon^{T} epsilon$ | # As per the convention we take epsilon to be a 2D column vector y = np.array([1.3, 2.5, 6.4, 8.1, 9.0]).reshape(-1, 1) y_hat = np.array([1.5, 2.0, 5.9, 8.5, 9.0]).reshape(-1, 1) epsilon = np.abs(y - y_hat) epsilon_square_sum1 = np.sum(epsilon**2) epsilon_square_sum2 = (epsilon.T @ epsilon).item() assert np.allclose(epsilon_square_sum1, epsilon_square_sum2) . . üëç works! . $(AB)^{T} = B^{T}A^{T}$ | A = np.random.randn(50, 10) B = np.random.randn(10, 20) ab_t = (A @ B).T b_t_a_t = B.T @ A.T assert np.allclose(ab_t, b_t_a_t) . . üôÑ Knew it already! . For a scalar $s$, $s = s^{T}$ | Derivative of scalar $s$ with respect to (yes!, I wrote wrt as full here üòÅ) vector $ theta$ $$ theta = begin{bmatrix} theta_{1} theta_{2} vdots theta{n} end{bmatrix}$$ $$ frac{ partial s}{ partial theta} = begin{bmatrix} frac{ partial s}{ partial theta_{1}} frac{ partial s}{ partial theta_{2}} frac{ partial s}{ partial theta_{3}} vdots frac{ partial s}{ partial theta_{n}} end{bmatrix} $$ | If $A$ is a matrix and $ theta$ is a vector, and $A theta$ is a scalar. Then $$ frac{ partial A theta}{ partial theta} = A^{T} $$ | ü§î Taking some similarity with $a theta$, where both $a$ and $ theta$ are scalar, I have an idea that it would be A. But shape of gradient would be $N times 1$, so $A^{T}$ is my guess before starting any calculations. . N = 20 # as A $ theta$ is scalar, so A.shape[0] should be 1. A = torch.randn((1, N)) theta = torch.randn((N, 1), requires_grad=True) scalar = A @ theta scalar.backward() assert torch.allclose(theta.grad, A.T) . . üëç all good . Assume $Z$ is a matrix of form $X^{T}X$, then $$ frac{ partial ( theta^{T}Z theta)}{ partial theta} = 2Z^{T} theta$$ | ü§î Let me again make a good guess before any calculation, if $ theta$ and $Z$ are both scaler, then the derivative would look like $2Z theta$. So my guess would $2Z theta$, which is equal to $2Z^{T} theta$ as both are $Z$ is symmetric. . X = torch.randn((N, N)) Z = X.T @ X theta = torch.randn((N, 1), requires_grad=True) scalar = theta.T @ Z @ theta scalar.backward() assert torch.allclose(theta.grad, 2 * Z.T @ theta) . . üëç good . Let&#39;s skip over the content of Rank topic for now. . The maximum rank possible for a matrix is $max(R, C)$ . But an interesting question would be ü§î, what is the minimum rank possible for a matrix, is it 0, is it 1? . Ans: Rank is zero, in case of zero matrix. . Just a leaving thought, if I would have been a developer of Numpy, I would not have allowed np.eye as the method for identity matrix. Better to use np.identity only. üòû . Linear Regression Introduction . Considering weight as a linear function of height: . $weight_{1} approx theta_{0} + theta_{1} * height_{1}$ | $weight_{2} approx theta_{0} + theta_{1} * height_{2}$ | $weight_{N} approx theta_{0} + theta_{1} * height_{N}$ | . Add extra columns of $1s$ for the bias term in $ theta$ . $$ W_{N times1} = X_{N times2} , theta_{2 times1} $$ where the feature matrix $X$, $X = begin{bmatrix} 1 &amp; height_{1} 1 &amp; height_{2} vdots &amp; vdots 1 &amp; height_{N} end{bmatrix}$ . $ theta_{0}$, Bias/Intercept term : (the value of $y$, when $x$ is set to zero) | $ theta_{1}$, Slope term : (the increase in $y$, when $x$ is increased by 1 unit) | . Generalized Linear Regression . $N$: Number of training samples | $M$: Number of features | . $$ begin{bmatrix} hat{y}_{1} hat{y}_{2} vdots hat{y}_{N} end{bmatrix} _{N times 1} = begin{bmatrix} 1 &amp; x_{1, 1} &amp; x_{1, 2} &amp; ldots &amp; x_{1, M} 1 &amp; x_{2, 1} &amp; x_{2, 2} &amp; ldots &amp; x_{2, M} vdots &amp; vdots &amp; vdots &amp; ldots &amp; vdots 1 &amp; x_{N, 1} &amp; x_{N, 2} &amp; ldots &amp; x_{N, M} end{bmatrix} _{N times (M + 1)} begin{bmatrix} theta_{0} theta_{1} vdots theta_{M} end{bmatrix} _{(M + 1) times 1} $$ $$ hat{y} = X theta $$ . Now, the task at our hand is to estimate &quot;good&quot; values of $ theta$, which will give &quot;good&quot; approximation to the actual values.But how do we decide if a set of values of $ theta$ is &quot;better&quot; than another value of $ theta$. We need a metric for evalution here. . Let $ epsilon_{i}$ be $y_{i} - hat{y}_{i}$, where $ epsilon_{i} sim mathcal{N} (0, sigma^{2})$. We are assuming that $ epsilon_{i}$ is coming from this normal distribution. . We want $| epsilon_{1}|$, $| epsilon_{2}|$, $| epsilon_{3}|$ ... , $| epsilon_{N}|$ to be small. . So we can try to minimize L2 norm (Squared Error) or L1 norm. . weight_height_df = pd.read_csv( &quot;assets/2022-02-17-machine-learning-quiz2-practice/weight-height.csv&quot; ) # take 30 points sampled_idx = np.random.choice(np.arange(len(weight_height_df)), size=30, replace=False) weight_height_df = weight_height_df.iloc[sampled_idx][[&quot;Height&quot;, &quot;Weight&quot;]].sort_values( by=[&quot;Height&quot;] ) def plot_func(theta0, theta1): x = weight_height_df[&quot;Height&quot;] y = weight_height_df[&quot;Weight&quot;] y_hat = theta0 + x * theta1 fig, ax = plt.subplots(figsize = (10, 8)) ax.scatter(x, y, label=&quot;Actual&quot;) ax.plot(x, y_hat, label=&quot;Pred&quot;, linestyle = &quot;--&quot;) ax.legend() ax = plt.gca() ax.set_ylim([50, 400]) mse_val = np.mean((y - y_hat)**2) ax.set_title(rf&quot;$ theta_{0}$={theta0}, $ theta_{1}$={theta1} MSE val: {mse_val:.3f}&quot;) interact( plot_func, theta0=widgets.FloatSlider(name = &quot;theta0 (bias)&quot;, value=-300, min=-1000, max=1000, step=1), theta1=widgets.FloatSlider(name = &quot;theta1 (slope)&quot;, value=7.5, min=-20, max=20, step=0.01), ) . . &lt;function __main__.plot_func(theta0, theta1)&gt; . . Note: Run the notebook in Colab to view the interactive plot above, where we manually change parameters (using sliders) and fit the line through training points with Mean Squared error as the guiding value. . Normal Equation . $$ y = X theta + epsilon$$ Objective: To minimize $ epsilon^{T} epsilon$ $$ epsilon^{T} epsilon = y y^{T} - 2 y^{T}X theta + theta^{T}X^{T}X theta$$ $$ frac{ partial ( epsilon^{T} epsilon)}{ partial theta} = -2X^{T}y + 2X^{T}X theta$$ (we use some of our results from previous chapter &quot;Maths for ML&quot;) . Setting it to zero, $$ theta^{*} = (X^{T}X)^{-1}X^{T}y$$ .",
            "url": "https://yadav-sachin.github.io/blog/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html",
            "relUrl": "/mlcourse2022/2022/02/17/machine-learning-quiz2-practice.html",
            "date": " ‚Ä¢ Feb 17, 2022"
        }
        
    
  
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://yadav-sachin.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://yadav-sachin.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}